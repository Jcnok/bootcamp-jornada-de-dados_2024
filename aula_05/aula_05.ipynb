{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180d6e20-eb17-42b4-a459-0d7fa37ce1be",
   "metadata": {},
   "source": [
    "# Projeto 01: Um Bilhão de Linhas: Desafio de Processamento de Dados com Python\n",
    "\n",
    "![imagem_01](./img/bootcamp.jpg)\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O objetivo deste projeto é demonstrar como processar eficientemente um arquivo de dados massivo contendo 1 bilhão de linhas (~14GB), especificamente para calcular estatísticas (Incluindo agregação e ordenação que são operações pesadas) utilizando Python. \n",
    "\n",
    "Este desafio foi inspirado no [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java, foi adaptado para o [bootcamp da jornada de dados 2024](https://www.jornadadedados2024.com.br/workshops)\n",
    "\n",
    "O arquivo de dados consiste em medições de temperatura de várias cidades(dados ficticios). Cada registro segue o formato `<string: nome da estação>;<double: medição>`, com a temperatura sendo apresentada com precisão de uma casa decimal.\n",
    "\n",
    "Aqui estão dez linhas de exemplo do arquivo:\n",
    "\n",
    "```\n",
    "Hamburg;12.0\n",
    "Bulawayo;8.9\n",
    "Palembang;38.8\n",
    "St. Johns;15.2\n",
    "Cracow;12.6\n",
    "Bridgetown;26.9\n",
    "Istanbul;6.2\n",
    "Roseau;34.4\n",
    "Conakry;31.2\n",
    "Istanbul;23.0\n",
    "```\n",
    "\n",
    "O desafio é desenvolver um programa Python capaz de ler esse arquivo e calcular a temperatura mínima, média (arredondada para uma casa decimal) e máxima para cada cidade, exibindo os resultados em uma tabela ordenada por nome da estação.\n",
    "\n",
    "| city      | min_temperature | mean_temperature | max_temperature |\n",
    "|--------------|-----------------|------------------|-----------------|\n",
    "| Abha         | -31.1           | 18.0             | 66.5            |\n",
    "| Abidjan      | -25.9           | 26.0             | 74.6            |\n",
    "| Abéché       | -19.8           | 29.4             | 79.9            |\n",
    "| Accra        | -24.8           | 26.4             | 76.3            |\n",
    "| Addis Ababa  | -31.8           | 16.0             | 63.9            |\n",
    "| ...          | ...             | ...              | ...             |\n",
    "| Zagreb       | -39.2           | 10.7             | 58.1            |\n",
    "| Zanzibar City| -26.5           | 26.0             | 75.2            |\n",
    "| Zürich       | -42.0           | 9.3              | 63.6            |\n",
    "| Ürümqi       | -42.1           | 7.4              | 56.7            |\n",
    "| İzmir        | -34.4           | 17.9             | 67.9            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea9c65-126b-402c-8004-0e250a6a5e55",
   "metadata": {},
   "source": [
    "## Sobre:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe1e3a-8b1d-45d2-86fd-1b42b901a804",
   "metadata": {},
   "source": [
    "Abaixo um índice como todas as etapas para resolução do projeto, desde informações sobre as configurações desktop, sobre a memória, ssd, como gerar os dados, dependências e o resultado do tempo de execução de cada script utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d20fa0-eb4a-4d7d-9e89-d83facdf96df",
   "metadata": {},
   "source": [
    "## índice\n",
    "\n",
    "<a id=\"voltar\"></a>\n",
    "\n",
    "1.  **[Decoradores](#ancora01)**\n",
    "2.  **[Scripts para gerar os dados.](#ancora02)**\n",
    "3.  **[Pandas - min, max e mean em 1 bilhão de linhas](#ancora03)**\n",
    "4.  **[Exercício 3: Filtragem de Logs por Severidade](#ancora04)**\n",
    "5.  **[Exercício 4: Validação de Dados de Entrada](#ancora05)**\n",
    "6.  **[Exercício 5: Detecção de Anomalias em Dados de Transações](#ancora05)**\n",
    "7.  **[Exercício 6: Contagem de Palavras em Textos](#ancora06)**\n",
    "8.  **[Exercício 7: Normalização de Dados](#ancora07)**\n",
    "9.  **[Exercício 8: Filtragem de Dados Faltantes](#ancora08)**\n",
    "10.  **[Exercício 9: Extração de Subconjuntos de Dados](#ancora09)**\n",
    "11. **[Exercício 10: Agregação de Dados por Categoria](#ancora10)**\n",
    "12. **[Exercício 11:  Leitura de Dados até Flag](#ancora11)**\n",
    "13. **[Exercício 12: Validação de Entrada](#ancora12)**\n",
    "14. **[Exercício 13: Consumo de API Simulado](#ancora13)**\n",
    "15. **[Exercício 14: Tentativas de Conexão](#ancora14)**\n",
    "16. **[Exercício 15: Processamento de Dados com Condição de Parada](#ancora15)**\n",
    "17. **[Desafio : Estruturas de Controle de Fluxo](#desafio)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7b87b-9ff0-4e17-bac4-ec9dc191add3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora01\"></a>\n",
    "## Decoradores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db8df0-beb2-4047-ae79-02521b633225",
   "metadata": {},
   "source": [
    "**O decorator timer serve para medir o tempo de execução de uma função e registrar o resultado em um arquivo CSV. Ele também imprime o tempo de execução no console.**\n",
    "\n",
    "**Porquê usar decorators?**\n",
    "* Nesse exemplo precisamos medir o tempo de cada uma das funções que serão criadas para determinar a melhor performance, dessa forma podemos criar uma função mais limpa e organizada onde iremos chamar o decorator para exibir e registrar as informações do tempo de execução da cada uma das funções, isso evita a duplicidade de código.\n",
    "\n",
    "* Funcionalidades:\n",
    "\n",
    "    - Mede o tempo de execução da função decorada.\n",
    "    - Formata o tempo em horas, minutos e segundos.\n",
    "    - Registra o nome da função e o tempo de execução em um arquivo CSV chamado tempos_execucao.csv.\n",
    "    - Imprime o nome da função e o tempo de execução no console.\n",
    "* Vantagens de usar decorators:\n",
    "\n",
    "    - Códigos mais concisos e reutilizáveis.\n",
    "    - Evita a duplicação de código para medição de tempo.\n",
    "    - Facilita a comparação do tempo de execução de diferentes funções.\n",
    "    - Permite a criação de logs de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f552d-c24f-4034-9248-f15c2044c199",
   "metadata": {},
   "source": [
    "* **Decorator para salvar os resultado em um csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7cc2e71-787d-4389-b0cb-ee625c8bccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/decorators.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/decorators.py \n",
    "#decorator para registrar o tempo de execução da função em um .csv\n",
    "import time\n",
    "import csv\n",
    "import threading  # Importe o módulo threading aqui\n",
    "\n",
    "def timer_to_csv(func):\n",
    "    def format_time(segundos: int): \n",
    "        \"\"\"\n",
    "        Formata os milisegundos em hora:minuto:segundo\n",
    "        \"\"\"\n",
    "        if segundos < 60:\n",
    "            return f\"{segundos:.3f} segundos\"\n",
    "        elif segundos < 3600:\n",
    "            minutos, segundos = divmod(segundos, 60)\n",
    "            return f\"{int(minutos)} minutos {int(segundos)} segundos\"\n",
    "        else:\n",
    "            horas, remainder = divmod(segundos, 3600)\n",
    "            minutos, segundos = divmod(remainder, 60)\n",
    "            if minutos == 0:\n",
    "                return f\"{int(horas)} horas {int(segundos)} segundos\"\n",
    "            else:\n",
    "                return f\"{int(horas)} horas {int(minutos)} minutos {int(segundos)} segundos\"\n",
    "    \n",
    "    def print_elapsed_time(start_time, finished_flag):\n",
    "        while not finished_flag.is_set():\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Tempo decorrido: {format_time(elapsed_time)}\", end=\"\\r\")\n",
    "            time.sleep(1)  # Atualiza a cada segundo\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Inicializa um sinalizador booleano para indicar se a função terminou\n",
    "        finished_flag = threading.Event()\n",
    "        \n",
    "        # Iniciar uma thread para imprimir o tempo decorrido em tempo real\n",
    "        thread = threading.Thread(target=print_elapsed_time, args=(start_time, finished_flag))\n",
    "        thread.start()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Tempo total de execução: {format_time(elapsed_time)}\")\n",
    "        \n",
    "        # Sinaliza que a função terminou\n",
    "        finished_flag.set()\n",
    "        \n",
    "        # Salvando o nome da função e o tempo de execução em um arquivo CSV\n",
    "        with open('data/tempos_execucao.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([func.__name__, format_time(elapsed_time)])\n",
    "        \n",
    "        return result    \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b229b-a9ad-40e5-8df7-c006e6c3d590",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91068054-2252-45e8-a9d1-12303e8ce8ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora02\"></a>\n",
    "## Scripts para gerar os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf73386-2b9e-4393-a9bf-06b1e88191d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script utilizando Pandas - not suport 1billion rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a77b5ee-6e1f-4e37-892a-bfa6e2cea78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib Pandas;\n",
    "!poetry add pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dda09dcd-7484-423a-9850-92bd201611f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a quantidade de linhas desejadas:  1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 289 ms, total: 3.35 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_data(num_rows):\n",
    "    \"\"\"\n",
    "    Gera dados aleatórios de temperatura para cidades e salva em um arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): Número de linhas desejadas para o DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Amostrar aleatoriamente os nomes das cidades existentes\n",
    "    city_names = pd.read_csv('data/weather_stations.csv', sep=';', header=None, skiprows=2, usecols=[0]).sample(num_rows, replace=True).iloc[:,0].tolist()\n",
    "\n",
    "    # Gerar temperaturas aleatórias\n",
    "    temperatures = [round(random.uniform(-89.2, 56.7), 1) for _ in range(num_rows)]\n",
    "\n",
    "    # Criar um DataFrame pandas com os dados gerados\n",
    "    df = pd.DataFrame({'city': city_names, 'temperature': temperatures})\n",
    "    \n",
    "    # Salvar o DataFrame em um arquivo CSV\n",
    "    df.to_csv('data/measurements_pandas.txt', sep=';', header=False, index=False)\n",
    "\n",
    "    # salvar o valor da quantidade de linhas solicitada   \n",
    "    with open('data/num_rows.txt', 'w') as file:\n",
    "        # Escrever o valor no arquivo\n",
    "        file.write(str(num_rows))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_rows = int(input(\"Digite a quantidade de linhas desejadas: \"))\n",
    "    generate_data(num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274703a-146f-472d-a0fa-3fadebf510bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scrip python para gerar dados aleatórios\n",
    "* **O script foi retirado do desafio [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java.**\n",
    "* **Algumas alterações foram realizadas, como por exemplo a temp máxima e mínima foram alterados para valores históricos reais.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ff1f98-314d-4e04-8e10-28da8764ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated max file size is:  7.4 GiB.\n",
      "Building test data...\n",
      "[==================================================] 100%\n",
      "Test data successfully written to data/measurements1.txt\n",
      "Actual file size:  7.4 GiB\n",
      "Elapsed time: 7 minutes 45 seconds\n",
      "Test data build complete.\n",
      "CPU times: user 7min 34s, sys: 11.1 s, total: 7min 45s\n",
      "Wall time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Script para gerar 1 bilhão de linhas com dados aleatórios.\n",
    "# Based on https://github.com/gunnarmorling/1brc/blob/main/src/main/java/dev/morling/onebrc/Createtempments.java\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def check_args(file_args):\n",
    "    \"\"\"\n",
    "    Sanity checks out input and prints out usage if input is not na positive integer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(file_args) != 2 or int(file_args[1]) <= 0:\n",
    "            raise Exception()\n",
    "    except:\n",
    "        print(\"Usage:  create_tempments.sh <positive integer number of records to create>\")\n",
    "        print(\"        You can use underscore notation for large number of records.\")\n",
    "        print(\"        For example:  1_000_000_000 for one billion\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def build_weather_city_name_list():\n",
    "    \"\"\"\n",
    "    Grabs the weather city names from example data provided in repo and dedups\n",
    "    \n",
    "    \"\"\"   \n",
    "    city_names = []\n",
    "    with open('data/weather_stations.csv', 'r') as file:\n",
    "\n",
    "        file_contents = file.read()\n",
    "    for city in file_contents.splitlines():\n",
    "        if \"#\" in city:\n",
    "            next\n",
    "        else:\n",
    "            city_names.append(city.split(';')[0])\n",
    "    return list(set(city_names))\n",
    "\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    Convert bytes to a human-readable format (e.g., KiB, MiB, GiB)\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KiB', 'MiB', 'GiB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def format_elapsed_time(seconds):\n",
    "    \"\"\"\n",
    "    Format elapsed time in a human-readable format\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.3f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes, seconds = divmod(seconds, 60)\n",
    "        return f\"{int(minutes)} minutes {int(seconds)} seconds\"\n",
    "    else:\n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        if minutes == 0:\n",
    "            return f\"{int(hours)} hours {int(seconds)} seconds\"\n",
    "        else:\n",
    "            return f\"{int(hours)} hours {int(minutes)} minutes {int(seconds)} seconds\"\n",
    "\n",
    "\n",
    "def estimate_file_size(weather_city_names, num_rows_to_create):\n",
    "    \"\"\"\n",
    "    Tries to estimate how large a file the test data will be\n",
    "    \"\"\"\n",
    "    total_name_bytes = sum(len(s.encode(\"utf-8\")) for s in weather_city_names)\n",
    "    avg_name_bytes = total_name_bytes / float(len(weather_city_names))\n",
    "\n",
    "    # avg_temp_bytes = sum(len(str(n / 10.0)) for n in range(-999, 1000)) / 1999\n",
    "    avg_temp_bytes = 4.400200100050025\n",
    "\n",
    "    # add 2 for separator and newline\n",
    "    avg_line_length = avg_name_bytes + avg_temp_bytes + 2\n",
    "\n",
    "    human_file_size = convert_bytes(num_rows_to_create * avg_line_length)\n",
    "\n",
    "    return f\"Estimated max file size is:  {human_file_size}.\"\n",
    "\n",
    "\n",
    "def build_test_data(weather_city_names, num_rows_to_create):\n",
    "    \"\"\"\n",
    "    Generates and writes to file the requested length of test data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    coldest_temp = -89.2\n",
    "    hottest_temp = 56.7\n",
    "    city_names_10k_max = random.choices(weather_city_names, k=10_000)\n",
    "    batch_size = 10000 # instead of writing line by line to file, process a batch of citys and put it to disk\n",
    "    chunks = num_rows_to_create // batch_size\n",
    "    print('Building test data...')\n",
    "\n",
    "    try:\n",
    "        with open(\"data/measurements.txt\", 'w') as file:\n",
    "            progress = 0\n",
    "            for chunk in range(chunks):\n",
    "                \n",
    "                batch = random.choices(city_names_10k_max, k=batch_size)\n",
    "                prepped_deviated_batch = '\\n'.join([f\"{city};{random.uniform(coldest_temp, hottest_temp):.1f}\" for city in batch]) # :.1f should quicker than round on a large scale, because round utilizes mathematical operation\n",
    "                file.write(prepped_deviated_batch + '\\n')\n",
    "\n",
    "                \n",
    "                # Update progress bar every 1%\n",
    "                if (chunk + 1) * 100 // chunks != progress:\n",
    "                    progress = (chunk + 1) * 100 // chunks\n",
    "                    bars = '=' * (progress // 2)\n",
    "                    sys.stdout.write(f\"\\r[{bars:<50}] {progress}%\")\n",
    "                    sys.stdout.flush()\n",
    "        sys.stdout.write('\\n')\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong. Printing error info and exiting...\")\n",
    "        print(e)\n",
    "        exit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    file_size = os.path.getsize(\"data/measurements.txt\")\n",
    "    human_file_size = convert_bytes(file_size)\n",
    " \n",
    "    print(\"Test data successfully written to data/measurements.txt\")\n",
    "    print(f\"Actual file size:  {human_file_size}\")\n",
    "    print(f\"Elapsed time: {format_elapsed_time(elapsed_time)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main program function\n",
    "    \"\"\"\n",
    "    num_rows_to_create = 500_000_000\n",
    "    weather_city_names = []\n",
    "    weather_city_names = build_weather_city_name_list()\n",
    "    print(estimate_file_size(weather_city_names, num_rows_to_create))\n",
    "    build_test_data(weather_city_names, num_rows_to_create)\n",
    "    print(\"Test data build complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fada2-9078-4ba4-b50e-f9b1e89faa88",
   "metadata": {},
   "source": [
    "#### Como executar o script:\n",
    "$`cd aula_05/src/` #para acessar a pasta dos scripts. \n",
    "\n",
    "$`python data_generate.py`#para criar os dados na pasta 'data/mesurements.txt'\n",
    "\n",
    "* **obs**: O script depende do arquivo 'data/weather_stations.csv' para gerar os dados das cidades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad06450-e001-4bd8-a2a3-0bc2e9547807",
   "metadata": {},
   "source": [
    "* **Agora com os dados de 1 bilhao gerados vamos ver quem se sai melhor para as configuracoes da minha maquina**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c14b73-eeef-4545-a1aa-11f872ae2d6b",
   "metadata": {},
   "source": [
    "<a id=\"ancora03\"></a>\n",
    "## Pandas - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81ac2551-f634-411a-bafe-70ceb59bdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a lib pandas.\n",
    "!poetry add pandas tabulate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38742de5-a0ce-4b7d-a260-08a3f2bc04d7",
   "metadata": {},
   "source": [
    "### Documentação do Script Python: Calculando Mínimo, Máximo e Média para um Bilhão de Linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034f5c-e3f2-419a-b89f-c455e50ab136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Este script calcula a temperatura mínima, máxima e média para cada cidade em um grande conjunto de dados contendo um bilhão de linhas. Ele utiliza as bibliotecas Pandas, multiprocessing e tqdm.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Leitura de dados:** \n",
    "    * Lê dados de um arquivo CSV (`data/measurements.txt`) com colunas nomeadas `cidade` e `temp`.\n",
    "    * Emprega fragmentação para processar os dados em partes menores e gerenciáveis.\n",
    "\n",
    "2. **Processamento paralelo:**\n",
    "    * Aproveita o multiprocessing para distribuir a carga de trabalho entre os núcleos de CPU disponíveis.\n",
    "    * Cada fragmento é processado independentemente pela função `process_chunk`.\n",
    "\n",
    "3. **Agregação:**\n",
    "    * Dentro de cada fragmento, a função `process_chunk` usa Pandas para:\n",
    "        * Agrupar dados por `cidade`.\n",
    "        * Calcular o `max`, `min` e `mean` da coluna `temp`.\n",
    "        * Redefinir o índice para obter um DataFrame limpo.\n",
    "\n",
    "4. **Agregação de resultados:**\n",
    "    * Resultados de fragmentos individuais são concatenados em um único DataFrame.\n",
    "    * Uma agregação final é realizada para calcular o `max`, `min` e `mean` geral para cada cidade.\n",
    "    * O DataFrame final é classificado por `cidade`.\n",
    "\n",
    "5. **Visualização do progresso:**\n",
    "    * A biblioteca tqdm fornece uma barra de progresso para visualizar o processamento dos fragmentos.\n",
    "\n",
    "**Principais características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** A fragmentação e o multiprocessing permitem lidar com dados massivos com eficiência.\n",
    "* **Desempenho otimizado:** A utilização de vários núcleos reduz significativamente o tempo de processamento.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de que a variável `filename` aponte para o local correto do arquivo de dados `data/measurements.txt`.\n",
    "* Ajuste `chunksize` dependendo da memória do seu sistema e dos recursos de processamento.\n",
    "\n",
    "**Documentação**: [Pandas](https://pandas.pydata.org/docs/)\n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/pandas_df.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fa4443-164f-4349-834b-a82a6827efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processamento do arquivo.\n",
      "Tempo total de execução: 1.379 segundos\n",
      "                   city   max   min   mean\n",
      "0              A Coruña  52.7 -88.1 -17.34\n",
      "1              A Yun Pa  50.6 -85.3 -21.25\n",
      "2              Aabenraa  47.7 -88.5 -37.76\n",
      "3                Aachen  38.8 -77.3 -33.72\n",
      "4                Aadorf  50.2 -85.2 -13.87\n",
      "...                 ...   ...   ...    ...\n",
      "41338   ’Tlat Bni Oukil  48.1 -81.7 -21.87\n",
      "41339     ’s-Gravendeel  54.0 -75.7  -7.58\n",
      "41340    ’s-Gravenzande  23.4 -82.3 -13.29\n",
      "41341     ’s-Heerenberg  55.0 -88.7 -19.00\n",
      "41342  ’s-Hertogenbosch  54.2 -73.9  -1.58\n",
      "\n",
      "[41343 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "!python src/pandas_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ecb0-8a95-4553-a846-5915a66c9748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script Pandas_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2f098-c6bd-4b4a-85d5-47ea486ed63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/pandas_df.py\n",
    "# Pandas=> script para caluclar min, max e mean em um bilhão de linhas.\n",
    "import pandas as pd \n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from utils.decorators import timer_to_csv  # Importa o decorador\n",
    "#from tabulate import tabulate\n",
    "\n",
    "CONCURRENCY = cpu_count()\n",
    "\n",
    "filename = \"data/measurements_pandas.txt\"  # Certifique-se de que este é o caminho correto para o arquivo\n",
    "\n",
    "def get_total_lines(num_rows_path):\n",
    "    with open(num_rows_path, 'r') as f:\n",
    "        total_lines = f.read()\n",
    "    return int(total_lines)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Agrega os dados dentro do chunk usando Pandas\n",
    "    aggregated = chunk.groupby('city')['temp'].agg(['max','min','mean']).reset_index()\n",
    "    return aggregated\n",
    "\n",
    "@timer_to_csv  # Aplica o decorador\n",
    "def create_df_with_pandas(filename, num_rows_path):\n",
    "    total_linhas = get_total_lines(num_rows_path)\n",
    "    chunksize = total_linhas // 10 + (1 if total_linhas % 10 else 0)  # Define o tamanho do chunk\n",
    "    with pd.read_csv(filename, sep=';', header=None, names=['city', 'temp'], chunksize=chunksize) as reader:\n",
    "        with Pool(CONCURRENCY) as pool:\n",
    "            results = []\n",
    "            for result in pool.imap(process_chunk, reader):\n",
    "                results.append(result)\n",
    "\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    final_aggregated_df = final_df.groupby('city').agg({\n",
    "        'max': 'max',\n",
    "        'min': 'min',\n",
    "        'mean': 'mean'\n",
    "        \n",
    "    }).reset_index().round(2).sort_values('city')\n",
    "\n",
    "    return final_aggregated_df\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    print(\"Iniciando o processamento do arquivo.\")\n",
    "    num_rows_path= \"data/num_rows.txt\"\n",
    "    filename = \"data/measurements_pandas.txt\"     \n",
    "    df = create_df_with_pandas(filename, num_rows_path)\n",
    "    #print(tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be5f4c-6870-4f2b-8403-3e4751da0eb1",
   "metadata": {},
   "source": [
    "## Polars - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e622fcb8-9f8f-4b34-bf90-e491ccf0bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a lib polars\n",
    "!poetry add polars -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f49575-3f35-4543-9591-b0059095b415",
   "metadata": {},
   "source": [
    "### Documentação do Script `polars_df.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfcd41-b9cb-4d9b-b15d-0c78c2f742e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Este script Python utiliza a biblioteca Polars para calcular de forma eficiente a temperatura mínima, máxima e média para cada cidade em um conjunto de dados grande (por exemplo, um bilhão de linhas). \n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Importações:**\n",
    "    * Importa a função `timer_to_csv` do módulo `utils.decorators`. \n",
    "    * Importa a biblioteca `polars` como `pl`.\n",
    "\n",
    "2. **Configuração do Polars:**\n",
    "    * Define o tamanho do chunk para processamento de streaming como 100.000 linhas usando `pl.Config.set_streaming_chunk_size(100000)`. Isso otimiza o uso de memória ao lidar com grandes conjuntos de dados.\n",
    "\n",
    "3. **Função `polars_df()`:**\n",
    "    * Esta função é decorada com `@timer_to_csv`, que mede o tempo de execução e salva os resultados em um arquivo CSV.\n",
    "    * Lê o arquivo CSV `data/measurements.txt` com as seguintes características:\n",
    "        * Separador: `;`\n",
    "        * Sem cabeçalho (`has_header=False`)\n",
    "        * Esquema de colunas: `{\"city\": pl.String, \"temp\": pl.Float64}`\n",
    "    * Agrupa os dados por `city`.\n",
    "    * Calcula as seguintes estatísticas para cada cidade:\n",
    "        * Temperatura máxima (`max_temp`)\n",
    "        * Temperatura mínima (`min_temp`)\n",
    "        * Temperatura média (`mean_temp`)\n",
    "    * Ordena os resultados por `city`.\n",
    "    * Coleta os resultados em um DataFrame Polars usando `collect(streaming=True)` para processamento eficiente de grandes conjuntos de dados.\n",
    "    * Retorna o DataFrame Polars resultante.\n",
    "\n",
    "4. **Execução principal:**\n",
    "    * Se o script for executado diretamente (e não importado como um módulo), a função `polars_df()` é chamada e o DataFrame resultante é impresso no console.\n",
    "\n",
    "**Principais características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** O Polars é otimizado para lidar com grandes conjuntos de dados, utilizando processamento em chunks e técnicas eficientes de gerenciamento de memória.\n",
    "* **Sintaxe expressiva:** A API do Polars permite realizar a análise de dados de forma concisa e legível.\n",
    "* **Desempenho:** O Polars é frequentemente mais rápido que o Pandas, especialmente em grandes conjuntos de dados.\n",
    "\n",
    "**Documentação:** [Polars](https://docs.pola.rs/#philosophy)\n",
    "\n",
    "Este script demonstra como o Polars pode ser usado para processar grandes conjuntos de dados de forma eficiente e calcular estatísticas básicas. \n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/polars_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef625854-08e9-4ab1-8dbc-1be8ab82df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução: 0.272 segundos\n",
      "shape: (41_343, 4)\n",
      "┌──────────────────┬──────────┬──────────┬────────────┐\n",
      "│ city             ┆ max_temp ┆ min_temp ┆ mean_temp  │\n",
      "│ ---              ┆ ---      ┆ ---      ┆ ---        │\n",
      "│ str              ┆ f64      ┆ f64      ┆ f64        │\n",
      "╞══════════════════╪══════════╪══════════╪════════════╡\n",
      "│ A Coruña         ┆ 52.7     ┆ -88.1    ┆ -13.647368 │\n",
      "│ A Yun Pa         ┆ 50.6     ┆ -85.3    ┆ -21.007407 │\n",
      "│ Aabenraa         ┆ 47.7     ┆ -88.5    ┆ -28.036842 │\n",
      "│ Aachen           ┆ 38.8     ┆ -77.3    ┆ -30.217391 │\n",
      "│ Aadorf           ┆ 50.2     ┆ -85.2    ┆ -13.6      │\n",
      "│ …                ┆ …        ┆ …        ┆ …          │\n",
      "│ ’Tlat Bni Oukil  ┆ 48.1     ┆ -81.7    ┆ -19.422222 │\n",
      "│ ’s-Gravendeel    ┆ 54.0     ┆ -75.7    ┆ -6.096429  │\n",
      "│ ’s-Gravenzande   ┆ 23.4     ┆ -82.3    ┆ -22.825    │\n",
      "│ ’s-Heerenberg    ┆ 55.0     ┆ -88.7    ┆ -5.705556  │\n",
      "│ ’s-Hertogenbosch ┆ 54.2     ┆ -73.9    ┆ -5.0       │\n",
      "└──────────────────┴──────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "!python src/polars_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfa647-57bc-4de3-9b26-3f25d2f43036",
   "metadata": {},
   "source": [
    "### Script Polars_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31de0d4-931d-4fd3-8353-373c41062cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/polars_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/polars_df.py\n",
    "# Polars => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "import polars as pl\n",
    "\n",
    "def get_total_lines(num_rows_path):\n",
    "    with open(num_rows_path, 'r') as f:\n",
    "        total_lines = f.read()\n",
    "    return int(total_lines)\n",
    "\n",
    "@timer_to_csv\n",
    "def polars_df(filename, num_rows_path):\n",
    "    total_linhas = get_total_lines(num_rows_path)\n",
    "    chunksize = total_linhas // 10 + (1 if total_linhas % 10 else 0)  # Define o tamanho do chunk\n",
    "\n",
    "    pl.Config.set_streaming_chunk_size(chunksize)\n",
    "\n",
    "    # Leitura do arquivo CSV e definição do schema\n",
    "    return (pl.scan_csv(filename, separator=\";\", has_header=False,\n",
    "                        schema={\"city\": pl.String, \"temp\": pl.Float64})\n",
    "                        .group_by(\"city\").agg(\n",
    "                                                 max_temp=pl.col(\"temp\").max(),\n",
    "                                                 min_temp=pl.col(\"temp\").min(),\n",
    "                                                 mean_temp=pl.col(\"temp\").mean()\n",
    "                                                ).sort(\"city\").collect(streaming=True)\n",
    "           )\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    num_rows_path= \"data/num_rows.txt\"\n",
    "    filename = \"data/measurements_pandas.txt\"  # Certifique-se de que este é o caminho correto para o arquivo\n",
    "    df = polars_df(filename, num_rows_path)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0fc93-52a7-4a33-9529-a3e8e78241c7",
   "metadata": {},
   "source": [
    "## Duckdb - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a4123a-14a4-4fd8-9336-c1a0dc1ab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add duckdb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7daa-a692-4d27-90f3-2c3535838cc5",
   "metadata": {},
   "source": [
    "### Documentação do Script `duckdb_df.py`\n",
    "\n",
    "Este script Python utiliza a biblioteca DuckDB para calcular de forma eficiente a temperatura mínima, máxima e média para cada cidade em um conjunto de dados grande (por exemplo, um bilhão de linhas). \n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Importações:**\n",
    "    * Importa a função `timer_to_csv` do módulo `utils.decorators`. \n",
    "    * Importa a biblioteca `duckdb`.\n",
    "\n",
    "2. **Função `duckdb_df()`:**\n",
    "    * Esta função é decorada com `@timer_to_csv`, que mede o tempo de execução e salva os resultados em um arquivo CSV.\n",
    "    * Executa uma consulta SQL no DuckDB para:\n",
    "        * Ler o arquivo CSV `data/measurements.txt` com as seguintes características:\n",
    "            * Detecção automática de tipo de dados desativada (`AUTO_DETECT=FALSE`)\n",
    "            * Separador: `;`\n",
    "            * Colunas: `city` (VARCHAR) e `temp` (DECIMAL)\n",
    "        * Agrupar os dados por `city`.\n",
    "        * Calcular as seguintes estatísticas para cada cidade:\n",
    "            * Temperatura máxima (`max_temp`)\n",
    "            * Temperatura mínima (`min_temp`)\n",
    "            * Temperatura média (`mean_temp`) - convertida para DECIMAL\n",
    "        * Ordenar os resultados por `city`.\n",
    "    * Exibe os resultados da consulta no console.\n",
    "\n",
    "3. **Execução principal:**\n",
    "    * Se o script for executado diretamente (e não importado como um módulo), a função `duckdb_df()` é chamada.\n",
    "\n",
    "**Documentação:** [Duckdb](https://duckdb.org/docs/)\n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/duckdb_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367987fa-6708-45bb-8261-95af03d5fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   city  max_temp  min_temp  mean_temp\n",
      "0              A Coruña      52.7     -88.1    -13.647\n",
      "1              A Yun Pa      50.6     -85.3    -21.007\n",
      "2              Aabenraa      47.7     -88.5    -28.037\n",
      "3                Aachen      38.8     -77.3    -30.217\n",
      "4                Aadorf      50.2     -85.2    -13.600\n",
      "...                 ...       ...       ...        ...\n",
      "41338   ’Tlat Bni Oukil      48.1     -81.7    -19.422\n",
      "41339     ’s-Gravendeel      54.0     -75.7     -6.096\n",
      "41340    ’s-Gravenzande      23.4     -82.3    -22.825\n",
      "41341     ’s-Heerenberg      55.0     -88.7     -5.706\n",
      "41342  ’s-Hertogenbosch      54.2     -73.9     -5.000\n",
      "\n",
      "[41343 rows x 4 columns]\n",
      "Tempo total de execução: 0.873 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/duckdb_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21464745-8551-4887-9327-de61861b0814",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script Duck_db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d06b82-0eb9-4580-9d56-1588a191dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/duckdb_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/duckdb_df.py\n",
    "# Duckdb => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "import duckdb\n",
    "\n",
    "def get_total_lines(num_rows_path):\n",
    "    with open(num_rows_path, 'r') as f:\n",
    "        total_lines = f.read()\n",
    "    return int(total_lines)\n",
    "\n",
    "@timer_to_csv\n",
    "def create_duckdb(filename,num_rows_path):\n",
    "    total_linhas = get_total_lines(num_rows_path)\n",
    "    chunksize = total_linhas // 10 + (1 if total_linhas % 10 else 0)  # Define o tamanho do chunk\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    query = f\"\"\"\n",
    "            SELECT city,\n",
    "                MAX(temp) AS max_temp,\n",
    "                MIN(temp) AS min_temp,\n",
    "                CAST(AVG(temp) AS DECIMAL()) AS mean_temp                \n",
    "            FROM read_csv(\"{filename}\", AUTO_DETECT=FALSE, sep=';', columns={{'city':VARCHAR, 'temp': 'DECIMAL'}})\n",
    "            GROUP BY city\n",
    "            ORDER BY city\n",
    "        \"\"\"\n",
    "    print(conn.execute(query).df())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/measurements_pandas.txt\" \n",
    "    num_rows_path= \"data/num_rows.txt\"\n",
    "    create_duckdb(filename,num_rows_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a87d1-37ef-4def-960f-1e885b6008cd",
   "metadata": {},
   "source": [
    "## Dask - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "99cd03e5-8f9e-4f2c-9397-91d7fc3dfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessário Instalar.\n",
    "!poetry add dask-expr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6225930b-ef90-4750-830f-171165204bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry remove dask-expr -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c9d87-b589-4c65-aa5d-d67653581258",
   "metadata": {},
   "source": [
    "### Documentação do Script Dask:\n",
    "\n",
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados massivo contendo um bilhão de linhas. Ele utiliza as bibliotecas Dask para otimizar o processamento e fornecer uma experiência interativa.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê dados de um arquivo CSV (`data/measurements.txt`) com colunas nomeadas `city` e `temp`.\n",
    "* Emprega a biblioteca Dask para ler o arquivo em um DataFrame particionado, permitindo processamento eficiente em grandes conjuntos de dados.\n",
    "\n",
    "**2. Processamento paralelo:**\n",
    "\n",
    "* O Dask gerencia o particionamento e o processamento paralelo do DataFrame em diferentes threads ou processos.\n",
    "* Cada partição é processada independentemente pela função `process_chunk`.\n",
    "\n",
    "**3. Agregação:**\n",
    "\n",
    "* Dentro de cada partição, a função `process_chunk` usa Pandas para:\n",
    "    * Agrupar dados por `city`.\n",
    "    * Calcular o `max`, `min` e `mean` da coluna `temp`.\n",
    "    * Redefinir o índice para obter um DataFrame limpo.\n",
    "\n",
    "**4. Agregação de resultados:**\n",
    "\n",
    "* Os resultados de cada partição são combinados em um único DataFrame.\n",
    "* Uma agregação final é realizada para calcular o `max`, `min` e `mean` geral para cada cidade.\n",
    "* O DataFrame final é ordenado por `city`.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** O Dask permite lidar com dados massivos de forma eficiente e escalável.\n",
    "* **Desempenho otimizado:** O processamento paralelo em partições reduz significativamente o tempo de processamento.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de que a variável `filename` aponte para o local correto do arquivo de dados `data/measurements.txt`.\n",
    "* Ajuste o tamanho da partição (`chunksize`) de acordo com a memória do seu sistema e os recursos de processamento.\n",
    "* Se necessário, configure o Dask para usar um cluster de computação para aumentar ainda mais o desempenho.\n",
    "\n",
    "**Documentação:** [Dask](https://docs.dask.org/en/stable/)\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd aula_05`\n",
    "2. **Execute o script:** `python src/dask_df.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcb3c5d-17b4-46dd-a2c0-cf61a1a8f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  temp             \n",
      "                   max   min   mean\n",
      "city                               \n",
      "A Coruña          52.7 -88.1 -13.65\n",
      "A Yun Pa          50.6 -85.3 -21.01\n",
      "Aabenraa          47.7 -88.5 -28.04\n",
      "Aachen            38.8 -77.3 -30.22\n",
      "Aadorf            50.2 -85.2 -13.60\n",
      "...                ...   ...    ...\n",
      "’Tlat Bni Oukil   48.1 -81.7 -19.42\n",
      "’s-Gravendeel     54.0 -75.7  -6.10\n",
      "’s-Gravenzande    23.4 -82.3 -22.82\n",
      "’s-Heerenberg     55.0 -88.7  -5.71\n",
      "’s-Hertogenbosch  54.2 -73.9  -5.00\n",
      "\n",
      "[41343 rows x 3 columns]\n",
      "Tempo total de execução: 0.795 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117e34d-6386-4149-a5c9-bab7aca7331f",
   "metadata": {},
   "source": [
    "### Script Dask_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f02dcf6-7633-4b3a-b68e-4d7a9c50a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/dask_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_df.py\n",
    "# Dask => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "@timer_to_csv\n",
    "def dask_df(filename):    \n",
    "    # Ler o arquivo txt diretamente em um DataFrame Dask\n",
    "    df = dd.read_csv(filename, delimiter=';', \n",
    "                     header=None, names=['city', 'temp'])\n",
    "    # min, max, e mean pela cidade ordenado pelo index\n",
    "    return print(df.groupby('city').\n",
    "                   agg({'temp': ['max','min','mean']}).\n",
    "                   compute().round(2).sort_index())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/measurements_pandas.txt\"\n",
    "    dask_df(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5a641-c514-4c94-a368-64424d857d9a",
   "metadata": {},
   "source": [
    "## Pyspark - min, max e mean em 1 bilhão de linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f73b4-0590-48b2-9bce-0409c9aea24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalação da lib\n",
    "!poetry add pyspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200eba8-d287-4b4e-b33e-4bf3b177877d",
   "metadata": {},
   "source": [
    "### Documentação do Script PySpark:\n",
    "\n",
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados massivo contendo um bilhão de linhas utilizando a biblioteca PySpark.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Inicialização da sessão Spark:**\n",
    "\n",
    "* Cria um objeto `SparkSession` para interagir com o cluster Spark.\n",
    "* Define o nome da aplicação como \"Temperature Analysis\".\n",
    "\n",
    "**2. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements.txt` diretamente em um DataFrame Spark.\n",
    "* Especifica que o arquivo não possui cabeçalho e usa ponto-e-vírgula como delimitador.\n",
    "* Define os nomes das colunas como \"City\" e \"Temperature\".\n",
    "\n",
    "**3. Conversão de tipo:**\n",
    "\n",
    "* Converte a coluna \"Temperature\" para o tipo numérico `float` para possibilitar cálculos.\n",
    "\n",
    "**4. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"City\".\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade utilizando funções Spark SQL.\n",
    "* Arredonda o valor médio da temperatura para duas casas decimais.\n",
    "\n",
    "**5. Ordenação:**\n",
    "\n",
    "* Ordena as estatísticas pela coluna \"City\".\n",
    "\n",
    "**6. Exibição de resultados:**\n",
    "\n",
    "* Imprime o DataFrame com as estatísticas na tela.\n",
    "\n",
    "**7. Encerramento da sessão Spark:**\n",
    "\n",
    "* Libera recursos utilizados pelo Spark.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento distribuído:** PySpark distribui o processamento por múltiplos nós em um cluster, permitindo lidar com conjuntos de dados massivos de maneira eficiente.\n",
    "* **Otimização para grandes dados:** PySpark oferece otimizações específicas para manipular grandes volumes de dados.\n",
    "* **Linguagem SQL familiar:** Usa Spark SQL para realizar consultas e transformações nos dados, facilitando a utilização para quem conhece SQL.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter o PySpark instalado e configurado corretamente.\n",
    "* O arquivo `data/measurements.txt` deve existir no caminho especificado.\n",
    "* A função `timer_to_csv` salva o tempo de execução em um arquivo CSV.\n",
    "\n",
    "**Documentação:** [pyspark](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd aula_05`\n",
    "2. **Execute o script:** `python src/pyspark_df.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44fe6fe-3fea-4d1a-b269-a1c4b91f8ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/26 23:58:59 WARN Utils: Your hostname, DESKTOP-AN64GAS resolves to a loopback address: 127.0.1.1; using 172.25.237.139 instead (on interface eth0)\n",
      "24/03/26 23:58:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/26 23:59:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+----------+---------------+---------------+---------------+                    \n",
      "|      City|Max Temperature|Min Temperature|Avg Temperature|\n",
      "+----------+---------------+---------------+---------------+\n",
      "|  A Coruña|           52.7|          -88.1|         -13.65|\n",
      "|  A Yun Pa|           50.6|          -85.3|         -21.01|\n",
      "|  Aabenraa|           47.7|          -88.5|         -28.04|\n",
      "|    Aachen|           38.8|          -77.3|         -30.22|\n",
      "|    Aadorf|           50.2|          -85.2|          -13.6|\n",
      "|   Aalborg|           54.8|          -84.7|         -19.55|\n",
      "|     Aalen|           50.3|          -80.1|         -13.52|\n",
      "|     Aaley|           47.3|          -88.6|         -26.93|\n",
      "|  Aalsmeer|           49.5|          -85.5|         -12.41|\n",
      "|     Aalst|           50.9|          -86.5|         -11.77|\n",
      "|    Aalten|           51.9|          -77.9|          -9.19|\n",
      "|     Aarau|           54.3|          -85.7|         -28.19|\n",
      "|    Aarhus|           51.2|          -88.3|          -17.5|\n",
      "|  Aarschot|           55.5|          -83.9|         -15.64|\n",
      "|    Aarsâl|           55.4|          -87.5|          -8.64|\n",
      "|Aartselaar|           55.7|          -78.6|         -12.89|\n",
      "|   Aasiaat|           36.7|          -70.1|         -19.88|\n",
      "|       Aba|           54.6|          -85.0|         -26.03|\n",
      "|    Abadan|           54.5|          -61.0|         -13.69|\n",
      "| Abadiânia|           46.9|          -80.0|          -14.2|\n",
      "+----------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tempo total de execução: 14.661 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/pyspark_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57d311-2eca-4461-ad43-a5c43b81a97d",
   "metadata": {},
   "source": [
    "### Script pyspark_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3950ab52-82b6-4f9a-8a6b-d9afefc2aa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pyspark_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pyspark_df.py\n",
    "# Pyspark => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min as spark_min, max as spark_max, avg as spark_avg, round as spark_round\n",
    "\n",
    "@timer_to_csv\n",
    "def pyspark_df(filename):     \n",
    "    # Inicializar uma sessão Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Temperature Analysis\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Ler o arquivo CSV diretamente em um DataFrame Spark\n",
    "    df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \";\").csv(filename) \\\n",
    "        .toDF(\"City\", \"Temperature\")\n",
    "    \n",
    "    # Converter a coluna 'Temperature' para tipo numérico\n",
    "    df = df.withColumn(\"Temperature\", col(\"Temperature\").cast(\"float\"))\n",
    "    \n",
    "    # Calcular estatísticas usando Spark SQL\n",
    "    statistics = df.groupBy(\"City\") \\\n",
    "        .agg(spark_max(\"Temperature\").alias(\"Max Temperature\"),\n",
    "             spark_min(\"Temperature\").alias(\"Min Temperature\"),             \n",
    "             spark_round(spark_avg(\"Temperature\"),2).alias(\"Avg Temperature\"))\n",
    "    \n",
    "    # Ordenar as estatísticas pela cidade\n",
    "    statistics_sorted = statistics.orderBy(\"City\")\n",
    "    \n",
    "    # Mostrar as estatísticas\n",
    "    return statistics_sorted.show()\n",
    "    \n",
    "    # Encerrar a sessão Spark\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/measurements_pandas.txt\"\n",
    "    pyspark_df(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2b54f-3cd0-4b33-9781-e1b2a8a2c66d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vaex - min, max e mean em 1 bilhão de linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b2e662f-d655-4479-85d8-595a8d1455df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add vaex -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6590b-9d9e-40c3-822d-9f34d2cdf25a",
   "metadata": {},
   "source": [
    "### Documentação do Script Vaex:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef27e66-4075-4479-894c-12f67c9c3c26",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um grande conjunto de dados, possivelmente contendo um bilhão de linhas, utilizando a biblioteca Vaex. O Vaex é uma biblioteca Python especializada no processamento eficiente de grandes tabelas de dados.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` utilizando a função `vaex.from_csv`.\n",
    "* Define os nomes das colunas como \"city\" e \"temp\" e o separador como ponto-e-vírgula (;).\n",
    "\n",
    "**2. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" usando `df.groupby(df['city'])`.\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade utilizando a função `agg` com uma lista de funções de agregação.\n",
    "* Arredonda o valor médio da temperatura para duas casas decimais usando o método `round(2)`.\n",
    "\n",
    "**3. Exibição de resultados:**\n",
    "\n",
    "* A função `print` exibe o DataFrame final contendo as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento eficiente de grandes dados:** O Vaex é otimizado para lidar com grandes volumes de dados na memória, permitindo cálculos rápidos em tabelas com bilhões de linhas.\n",
    "* **Operações em memória:** Realiza a maioria das operações na memória principal, evitando acesso frequente ao disco e melhorando a performance.\n",
    "* **Uso intuitivo:** Oferece uma interface similar ao Pandas, facilitando a migração de scripts existentes.\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python vaex_df.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bdd7daa-9809-4139-8c6c-774a09eda3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#       city              temp_max    temp_min    temp_mean\n",
      "0       A Coruña          52.7        -88.1       -13.64736842105263\n",
      "1       A Yun Pa          50.6        -85.3       -21.007407407407406\n",
      "2       Aabenraa          47.7        -88.5       -28.036842105263155\n",
      "3       Aachen            38.8        -77.3       -30.21739130434783\n",
      "4       Aadorf            50.2        -85.2       -13.6\n",
      "...     ...               ...         ...         ...\n",
      "41,338  ’Tlat Bni Oukil   48.1        -81.7       -19.42222222222222\n",
      "41,339  ’s-Gravendeel     54.0        -75.7       -6.096428571428571\n",
      "41,340  ’s-Gravenzande    23.4        -82.3       -22.825\n",
      "41,341  ’s-Heerenberg     55.0        -88.7       -5.705555555555555\n",
      "41,342  ’s-Hertogenbosch  54.2        -73.9       -5.0\n",
      "Tempo total de execução: 1.045 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/vaex_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090a456-00d8-45aa-92c2-d9a348b8f384",
   "metadata": {},
   "source": [
    "### Script vaex_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc87d1a2-852f-4c65-bd6e-215faf967407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/vaex_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/vaex_df.py\n",
    "# Vaex => script para caluclar min, max e mean.\n",
    "from utils.decorators import timer_to_csv\n",
    "import vaex\n",
    "@timer_to_csv\n",
    "def vaex_df(filename):\n",
    "    # Leitura do arquivo CSV utilizando Vaex\n",
    "    df = vaex.from_csv(filename, sep=';', header=None, names=['city', 'temp'])\n",
    "\n",
    "    # Cálculo das estatísticas\n",
    "    combined_results = df.groupby(df['city']).agg({'temp': ['max', 'min', 'mean']})\n",
    "\n",
    "    # Ordenar por 'city'\n",
    "    combined_results = combined_results.sort(by='city')\n",
    "    \n",
    "    # Exibição dos resultados\n",
    "    return print(combined_results)\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/measurements_pandas.txt\"\n",
    "    create_vaex(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e549a-a6eb-47fb-af6c-273174f288c4",
   "metadata": {},
   "source": [
    "## cudf com pandas via GPU - min, max e mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c13e7-9825-4fa1-8f83-f38d469bfa91",
   "metadata": {},
   "source": [
    "### Para utilizar precisa ter uma GPU compatível e se possui DriveToolKit instalado conforme o link da [Nvidia](https://developer.nvidia.com/cuda-gpus#compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc3bc5-e731-4f50-b003-294fc982ecef",
   "metadata": {},
   "source": [
    "* **Confira a versão cuda instalado com o comando abaixo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee832a0-7f24-4968-b5d2-7a3122d6ea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 23:59:39 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:03:00.0  On |                  N/A |\n",
      "|  0%   43C    P8             15W /  200W |     668MiB /   8192MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        38      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c319fba-02a7-44be-99df-a52833f8fdda",
   "metadata": {},
   "source": [
    "* **No meu caso estou com a versão 12+ com essa informação acesse o site para escolher a versão correta no site:**\n",
    "[RAPIDS-Nvidi](https://docs.rapids.ai/install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab3c819-0818-4ee5-b9a0-4ab180eb3384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "#verificando a versão do python para esse estudo.\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74a151a-18d2-4e45-aa01-6908082eca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jcnok/bootcamps/mytests/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "#Necessário fazer upgrade do pip\n",
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40db29da-70d6-40bb-a794-462547390d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-expr 1.0.5 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalação somente pelo pip. não funciona com poetry add.\n",
    "!pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com cudf-cu12 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee476b7-5b26-496a-b13d-af82c876c767",
   "metadata": {},
   "source": [
    "#### Após a instalação é necessário reiniciar o kernel do jupyter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944e509b-adab-469c-9d48-41314e54020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reinicia o kernel jupyter\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4958c6-2438-4799-8da9-35c76244cbd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cudf.pandas extension is already loaded. To reload it, use:\n",
      "  %reload_ext cudf.pandas\n",
      "Tempo decorrido: 0.001 segundos\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Coruña</td>\n",
       "      <td>52.7</td>\n",
       "      <td>-88.1</td>\n",
       "      <td>-13.647368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Yun Pa</td>\n",
       "      <td>50.6</td>\n",
       "      <td>-85.3</td>\n",
       "      <td>-21.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aabenraa</td>\n",
       "      <td>47.7</td>\n",
       "      <td>-88.5</td>\n",
       "      <td>-28.036842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aachen</td>\n",
       "      <td>38.8</td>\n",
       "      <td>-77.3</td>\n",
       "      <td>-30.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aadorf</td>\n",
       "      <td>50.2</td>\n",
       "      <td>-85.2</td>\n",
       "      <td>-13.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41338</th>\n",
       "      <td>’Tlat Bni Oukil</td>\n",
       "      <td>48.1</td>\n",
       "      <td>-81.7</td>\n",
       "      <td>-19.422222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41339</th>\n",
       "      <td>’s-Gravendeel</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-75.7</td>\n",
       "      <td>-6.096429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41340</th>\n",
       "      <td>’s-Gravenzande</td>\n",
       "      <td>23.4</td>\n",
       "      <td>-82.3</td>\n",
       "      <td>-22.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41341</th>\n",
       "      <td>’s-Heerenberg</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-88.7</td>\n",
       "      <td>-5.705556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41342</th>\n",
       "      <td>’s-Hertogenbosch</td>\n",
       "      <td>54.2</td>\n",
       "      <td>-73.9</td>\n",
       "      <td>-5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41343 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city   max   min       mean\n",
       "0              A Coruña  52.7 -88.1 -13.647368\n",
       "1              A Yun Pa  50.6 -85.3 -21.007407\n",
       "2              Aabenraa  47.7 -88.5 -28.036842\n",
       "3                Aachen  38.8 -77.3 -30.217391\n",
       "4                Aadorf  50.2 -85.2 -13.600000\n",
       "...                 ...   ...   ...        ...\n",
       "41338   ’Tlat Bni Oukil  48.1 -81.7 -19.422222\n",
       "41339     ’s-Gravendeel  54.0 -75.7  -6.096429\n",
       "41340    ’s-Gravenzande  23.4 -82.3 -22.825000\n",
       "41341     ’s-Heerenberg  55.0 -88.7  -5.705556\n",
       "41342  ’s-Hertogenbosch  54.2 -73.9  -5.000000\n",
       "\n",
       "[41343 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução: 0.818 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# CuDF_Pandas=> script para caluclar min, max e mean em um bilhão de linhas na GPU rtx 3060ti.\n",
    "# Necessário possuir uma GPU compativel e instalar o pacote RAPIDS da Nvidia.\n",
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "from src.utils.decorators import timer_to_csv\n",
    "@timer_to_csv\n",
    "def cudf_pandas_df():\n",
    "    # Defina o caminho para o seu arquivo CSV\n",
    "    file_path = \"data/measurements_pandas.txt\"\n",
    "    \n",
    "    # Tamanho do chunk (você pode ajustar conforme necessário)\n",
    "    chunk_size = 100_000_000  # por exemplo, 1 milhão de linhas por chunk\n",
    "    \n",
    "    # Inicialize um DataFrame vazio para armazenar os resultados\n",
    "    results_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop através do arquivo em chunks e calcule as estatísticas\n",
    "    for chunk in pd.read_csv(file_path,sep=';', header=None, names=['city', 'temp'], chunksize=chunk_size):\n",
    "        # Calcular as estatísticas por grupo\n",
    "        grouped_stats = chunk.groupby('city').agg({'temp': ['max','min','mean']}).reset_index()\n",
    "        # Concatenar os resultados ao DataFrame principal\n",
    "        results_df = pd.concat([results_df, grouped_stats], ignore_index=True)\n",
    "    # Removendo o level city    \n",
    "    results_df = results_df.droplevel(0,axis=1)\n",
    "    # Renomeando o level 0 para city\n",
    "    results_df.rename(columns={'':'city'}, inplace=True)\n",
    "    # Fazendo o groupby geral.\n",
    "    results_df.groupby('city').agg({ 'max':'max', 'min':'min', 'mean':'mean'}).round(2).sort_values('city')    \n",
    "    # Resultados finais\n",
    "    return display(results_df)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    print(cudf_pandas_df())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ccf0a-673e-41db-a54f-973aacb0678d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentação do Script cudf_pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e02e54-5975-4862-b7fe-275f37311550",
   "metadata": {},
   "source": [
    "* **Para a leitura para 10 milhões de linha foi extremamente rápido, porém para 1 bilhão a memória não suportou.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c2c6b-e636-4db9-8a26-e648edd1f6b8",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados utilizando a biblioteca CuDF. O CuDF é uma biblioteca similar ao Pandas, porém otimizada para processamento em GPUs, possibilitando um desempenho significativamente superior.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "Este script **não é recomendado** para processar um bilhão de linhas, pois a memória da GPU pode ser insuficiente. Para conjuntos de dados massivos, utilize a versão do script CuDF otimizada para Dask.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` usando `cudf.read_csv`.\n",
    "* Define `header=None` pois o arquivo não possui cabeçalho e `sep=;` como separador.\n",
    "* Cria as colunas \"city\" e \"temp\".\n",
    "\n",
    "**2. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" com `df.groupby('city')`.\n",
    "* Calcula o mínimo, máximo e média da temperatura para cada cidade usando `agg` com uma lista de funções de agregação.\n",
    "\n",
    "**3. Ordenação:**\n",
    "\n",
    "* Ordena o DataFrame pelo índice (cidade) usando `sort_index()`.\n",
    "\n",
    "**4. Exibição de resultados:**\n",
    "\n",
    "* Imprime o DataFrame final com as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento em GPU:** O CuDF utiliza a GPU para realizar cálculos, proporcionando um aumento significativo de performance em comparação ao Pandas em CPUs.\n",
    "* **Interface familiar:** A API do CuDF é similar ao Pandas, facilitando a migração de scripts existentes.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter o CuDF instalado e drivers Nvidia atualizados.\n",
    "* O arquivo `data/measurements_pandas.txt` deve existir no caminho especificado.\n",
    "* A memória da GPU pode ser insuficiente para processar grandes conjuntos de dados.\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python cudf_pandas_df.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728394c5-bccc-4f4d-8587-9499701918dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  temp                 \n",
      "                   min   max       mean\n",
      "city                                   \n",
      "A Coruña         -88.1  52.7 -13.647368\n",
      "A Yun Pa         -85.3  50.6 -21.007407\n",
      "Aabenraa         -88.5  47.7 -28.036842\n",
      "Aachen           -77.3  38.8 -30.217391\n",
      "Aadorf           -85.2  50.2 -13.600000\n",
      "...                ...   ...        ...\n",
      "’Tlat Bni Oukil  -81.7  48.1 -19.422222\n",
      "’s-Gravendeel    -75.7  54.0  -6.096429\n",
      "’s-Gravenzande   -82.3  23.4 -22.825000\n",
      "’s-Heerenberg    -88.7  55.0  -5.705556\n",
      "’s-Hertogenbosch -73.9  54.2  -5.000000\n",
      "\n",
      "[41343 rows x 3 columns]\n",
      "Tempo total de execução: 0.690 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/cudf_pandas_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c806c-f11f-49c2-bafe-1a1321c70081",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script cudf_pandas_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9a4784-5d7a-4e9f-abb4-b2e69fee5fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cudf_pandas_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cudf_pandas_df.py\n",
    "# Script usando cudf == pandas mas rápido pq usa a GPU, porém a mem não suport procesar 1bilhão.\n",
    "import cudf\n",
    "from utils.decorators import timer_to_csv\n",
    "@timer_to_csv\n",
    "def cudf_pandas(filename):\n",
    "    \n",
    "    # Carregar o arquivo CSV e criar os cabeçalhos 'city' e 'temp'\n",
    "    df = cudf.read_csv(filename, header=None, sep=';', names=['city', 'temp'])\n",
    "    \n",
    "    # Agrupar por 'city' e calcular o 'min', 'max' e 'mean' da coluna 'temp'\n",
    "    grouped_df = df.groupby('city').agg({'temp': ['min', 'max', 'mean']})\n",
    "    \n",
    "    # Ordenar o DataFrame pelo índice (city)\n",
    "    result = grouped_df.sort_index()     \n",
    "    \n",
    "    # retorna o resultado\n",
    "    return print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data/measurements_pandas.txt\"\n",
    "    cudf_pandas(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a47fc4a6-f170-4067-9af5-977bcb0f4171",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/bootcamps/bootcamp-jornada-de-dados_2024/.venv/lib/python3.11/site-packages/dask_cudf/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_delayed\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backends\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __git_commit__, __version__\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, Series, concat, from_cudf, from_dask_dataframe\n",
      "File \u001b[0;32m~/bootcamps/bootcamp-jornada-de-dados_2024/.venv/lib/python3.11/site-packages/dask_cudf/backends.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_string_dtype\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnvtx_annotation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dask_cudf_nvtx_annotate\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, Index, Series\n\u001b[1;32m     49\u001b[0m get_parallel_type\u001b[38;5;241m.\u001b[39mregister(cudf\u001b[38;5;241m.\u001b[39mDataFrame, \u001b[38;5;28;01mlambda\u001b[39;00m _: DataFrame)\n\u001b[1;32m     50\u001b[0m get_parallel_type\u001b[38;5;241m.\u001b[39mregister(cudf\u001b[38;5;241m.\u001b[39mSeries, \u001b[38;5;28;01mlambda\u001b[39;00m _: Series)\n",
      "File \u001b[0;32m~/bootcamps/bootcamp-jornada-de-dados_2024/.venv/lib/python3.11/site-packages/dask_cudf/core.py:714\u001b[0m\n\u001b[1;32m    693\u001b[0m     name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_cudf-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tokenize(data, npartitions \u001b[38;5;129;01mor\u001b[39;00m chunksize))\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[1;32m    695\u001b[0m         data,\n\u001b[1;32m    696\u001b[0m         npartitions\u001b[38;5;241m=\u001b[39mnpartitions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    703\u001b[0m from_cudf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    704\u001b[0m     textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03m        Create a :class:`.DataFrame` from a :class:`cudf.DataFrame`.\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m        This function is a thin wrapper around\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m        :func:`dask.dataframe.from_pandas`, accepting the same\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m        arguments (described below) excepting that it operates on cuDF\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m        rather than pandas objects.\\n\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     )\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[43mtextwrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdedent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__doc__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m )\n\u001b[1;32m    718\u001b[0m \u001b[38;5;129m@_dask_cudf_nvtx_annotate\u001b[39m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dask_dataframe\u001b[39m(df):\n\u001b[1;32m    720\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    Convert a Dask :class:`dask.dataframe.DataFrame` to a Dask-cuDF\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    one.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m    dask_cudf.DataFrame : A new Dask collection backed by cuDF objects\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/textwrap.py:435\u001b[0m, in \u001b[0;36mdedent\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Look for the longest leading string of spaces and tabs common to\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# all lines.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m margin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43m_whitespace_only_re\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m indents \u001b[38;5;241m=\u001b[39m _leading_whitespace_re\u001b[38;5;241m.\u001b[39mfindall(text)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indent \u001b[38;5;129;01min\u001b[39;00m indents:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dask_cudf as dc\n",
    "\n",
    "# Carregar o arquivo csv em um dataframe dask_cudf\n",
    "df = dc.read_csv('data/measurements.txt',sep=';', header=None, names=['city', 'temp'], dtype=['str', 'float32'], blocksize='1024 Mib')\n",
    "\n",
    "# Agrupar pela coluna 'city' e calcular min, max e mean da coluna 'temp'\n",
    "grouped_df = df.groupby('city').agg({'temp': ['min', 'max', 'mean']}).compute()\n",
    "\n",
    "# Ordenar pelo 'city'\n",
    "sorted_df = grouped_df.sort_index()\n",
    "\n",
    "# Imprimir as 5 primeiras e últimas linhas\n",
    "display(sorted_df)\n",
    "#print(sorted_df.tail(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c59e4-a3d7-4caa-a681-0e05cae2b033",
   "metadata": {},
   "source": [
    "## Dask_cudf com GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718461e-b026-4ea5-be22-3836c6f8a000",
   "metadata": {},
   "source": [
    "#### Para utilização do dasck_cudf será necessária a instalação via pip conforme abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391c04c-ecc0-49f5-adbc-5be0e6977e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib via pip, via poetry apresentou erros.\n",
    "!pip install --no-cache-dir --extra-index-url=https://pypi.nvidia.com dask-cudf-cu12 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cca96e-88cd-4d2a-8767-402059ce0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib dask_cuda\n",
    "!pip install dask-cuda -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685f397b-fcd1-4b90-a80b-3d3f981eb23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reinicia o kernel jupyter\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cd094-5ec9-4d16-b0cf-54dfb4f8ba66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentação do Script Dask-CuDF com Blocksize 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048e863-feec-42e8-b894-679a90bd7ea0",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados de um bilhão de linhas utilizando Dask-CuDF, combinando o poder do processamento paralelo do Dask com a aceleração por GPU do CuDF.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Configuração do Cluster GPU:**\n",
    "\n",
    "* Cria um cluster local com um único nó utilizando a GPU disponível (`LocalCUDACluster`).\n",
    "* Inicia um cliente Dask conectado ao cluster para distribuir o processamento.\n",
    "\n",
    "**2. Leitura dos Dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` usando `dc.read_csv`.\n",
    "* Define as seguintes opções:\n",
    "    - `sep=';'` para indicar o separador de colunas.\n",
    "    - `header=None` pois o arquivo não possui cabeçalho.\n",
    "    - `names=['city', 'temp']` para nomear as colunas.\n",
    "    - `dtype={'city': 'str', 'temp': 'float32'}` para especificar os tipos de dados.\n",
    "    - `blocksize='1024 MiB'` para dividir o arquivo em partições de 1024 MiB para processamento paralelo.\n",
    "\n",
    "**3. Cálculo de Estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" usando `df.groupby('city')`.\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade usando `agg` com uma lista de funções de agregação.\n",
    "* Computa os resultados finais com `compute()`.\n",
    "* Ordena o DataFrame pelo índice (cidade) usando `sort_index()`.\n",
    "\n",
    "**4. Exibição de Resultados:**\n",
    "\n",
    "* Imprime o DataFrame final contendo as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento em GPU Distribuído:** Combina Dask para paralelismo com CuDF para aceleração por GPU.\n",
    "* **Blocksize para Particionamento:** Divide o arquivo em partições para melhor processamento paralelo.\n",
    "* **Interface Familiar:** API similar ao Pandas, facilitando a adaptação.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter Dask, Dask-CUDA e CuDF instalados.\n",
    "* A GPU deve ter memória suficiente para processar as partições de 1024 MiB.\n",
    "* O arquivo `data/measurements_pandas.txt` deve existir no caminho especificado.\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python dask_cudf_1024.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17202a20-0707-4b56-9533-50020e4b171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcnok/bootcamps/mytests/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34397 instead\n",
      "  warnings.warn(\n",
      "/home/jcnok/bootcamps/mytests/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n",
      "                       temp                      \n",
      "                        max        min       mean\n",
      "city                                             \n",
      "A Coruña          52.700001 -88.099998 -13.647368\n",
      "A Yun Pa          50.599998 -85.300003 -21.007410\n",
      "Aabenraa          47.700001 -88.500000 -28.036840\n",
      "Aachen            38.799999 -77.300003 -30.217391\n",
      "Aadorf            50.200001 -85.199997 -13.600000\n",
      "...                     ...        ...        ...\n",
      "’Tlat Bni Oukil   48.099998 -81.699997 -19.422223\n",
      "’s-Gravendeel     54.000000 -75.699997  -6.096428\n",
      "’s-Gravenzande    23.400000 -82.300003 -22.825000\n",
      "’s-Heerenberg     55.000000 -88.699997  -5.705555\n",
      "’s-Hertogenbosch  54.200001 -73.900002  -4.999999\n",
      "\n",
      "[41343 rows x 3 columns]\n",
      "Tempo total de execução: 5.748 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_cudf_1024.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37452434-65a6-43ac-810f-17489ac9e589",
   "metadata": {},
   "source": [
    "### Script Dasck_cudf_1024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df064c24-11ad-4298-9d30-69ea19d15698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/dask_cudf_1024.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_cudf_1024.py\n",
    "# dask_cudf com blocksize 1024 => script para calcular min, max e mean em um bilhão de linhas usando a gpu rtx 3060ti.\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from utils.decorators import timer_to_csv\n",
    "import dask_cudf as dc\n",
    "\n",
    "@timer_to_csv\n",
    "def dask_cudf_1024(filename):\n",
    "    # Definindo o cluster para usar gpu\n",
    "    with LocalCUDACluster() as cluster, Client(cluster) as client:\n",
    "        # Carregar o arquivo csv em um dataframe dask_cudf\n",
    "        df = dc.read_csv(filename,\n",
    "                         sep=';', header=None,\n",
    "                         names=['city', 'temp'], \n",
    "                         dtype={'city': 'str', 'temp': 'float32'},\n",
    "                         blocksize='1024 MiB')\n",
    "        \n",
    "        # Agrupar pela coluna 'city' e calcular min, max e mean da coluna 'temp'\n",
    "        result = df.groupby('city').agg({'temp': ['max','min','mean']}).compute().sort_index()\n",
    "        print(result)\n",
    "if __name__ == \"__main__\":\n",
    "    filename= 'data/measurements_pandas.txt'\n",
    "    dask_cudf_1024(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca0eee-5817-4d62-8913-c5cfec1500b4",
   "metadata": {},
   "source": [
    "### Script Dask_cudf_256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e030d72-2b7f-4edf-a3d6-475a70e92766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/dask_cudf_256.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_cudf_256.py\n",
    "# dask_cudf com blocksize 256mb => script para caluclar min, max e mean em um bilhão de linhas usando a gpu rtx 3060ti.\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from utils.decorators import timer_to_csv\n",
    "import dask_cudf as dc\n",
    "\n",
    "@timer_to_csv\n",
    "def dask_cudf_256(filename):\n",
    "    # Definindo o cluster para usar gpu\n",
    "    with LocalCUDACluster() as cluster, Client(cluster) as client:\n",
    "        # Carregar o arquivo csv em um dataframe dask_cudf\n",
    "        df = dc.read_csv(filename,\n",
    "                         sep=';', header=None,\n",
    "                         names=['city', 'temp'], \n",
    "                         dtype={'city': 'str', 'temp': 'float32'},\n",
    "                         blocksize='256 MiB')\n",
    "        \n",
    "        # Agrupar pela coluna 'city' e calcular min, max e mean da coluna 'temp'\n",
    "        result = df.groupby('city').agg({'temp': ['max','min','mean']}).compute().sort_index()\n",
    "        print(result)\n",
    "if __name__ == \"__main__\":\n",
    "    filename= 'data/measurements_pandas.txt'\n",
    "    dask_cudf_256(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb4cdda-35e1-402e-b29e-4e6ab2455b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcnok/bootcamps/mytests/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n",
      "                       temp                      \n",
      "                        max        min       mean\n",
      "city                                             \n",
      "A Coruña          52.700001 -88.099998 -13.647368\n",
      "A Yun Pa          50.599998 -85.300003 -21.007410\n",
      "Aabenraa          47.700001 -88.500000 -28.036840\n",
      "Aachen            38.799999 -77.300003 -30.217391\n",
      "Aadorf            50.200001 -85.199997 -13.600000\n",
      "...                     ...        ...        ...\n",
      "’Tlat Bni Oukil   48.099998 -81.699997 -19.422223\n",
      "’s-Gravendeel     54.000000 -75.699997  -6.096428\n",
      "’s-Gravenzande    23.400000 -82.300003 -22.825000\n",
      "’s-Heerenberg     55.000000 -88.699997  -5.705555\n",
      "’s-Hertogenbosch  54.200001 -73.900002  -4.999999\n",
      "\n",
      "[41343 rows x 3 columns]\n",
      "Tempo total de execução: 5.131 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_cudf_256.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd4657-f16a-4687-8bd7-7dc1e65a2c35",
   "metadata": {},
   "source": [
    "* **Alterando para 256 MiB a performance melhorou, mas ainda assim bem longe de superar o DuckDB.**\n",
    "* **Conforme o benchmarch da documentação ele tem capacidade de superar o sgdb. Talvez alguma configuração pois ainda não li toda a documentação.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac218d-60f5-4f0c-bb05-886f74fef005",
   "metadata": {},
   "source": [
    "## Gerando um Gráfico dos Resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67997a8d-42a5-40bd-928a-02614905e4fd",
   "metadata": {},
   "source": [
    "#### Segue abaixo o resultado do ranking geral de todos os testes realizados para calcular o min, max e média ordenado pela cidade em um bilhão de linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5469e-a8c2-4139-83ca-5a6b57519cd1",
   "metadata": {},
   "source": [
    "### Resultado realizado para calcular o min, max e média ordenado pela cidade em um Milhão de linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a578ea-1b3d-428a-a23c-abedc2d7108c",
   "metadata": {},
   "source": [
    "![png](./img/ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22e866-e7f7-49b3-930b-408535bcdcc7",
   "metadata": {},
   "source": [
    "* **obs:** Vale ressaltar que esse resultado só serve de parâmetro para minha máquina, com base nos scripts gerados aqui, certamente cabem espaços para otimização e em outros cenários os resultados podem ser bem divergentes.\n",
    "* segue as configurações da minha máquina abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c7c2e-4c8e-41ee-bcc1-c0834c01cbad",
   "metadata": {},
   "source": [
    "#### Memória:\n",
    "Memory block size:       128M\n",
    "Total online memory:      12G\n",
    "Total offline memory:      0B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d37301-3900-45b5-b5a0-09c6b6f52210",
   "metadata": {},
   "source": [
    "#### CPU:\n",
    "Architecture:            x86_64\n",
    "  CPU op-mode(s):        32-bit, 64-bit\n",
    "  Address sizes:         46 bits physical, 48 bits virtual\n",
    "  Byte Order:            Little Endian\n",
    "CPU(s):                  24\n",
    "  On-line CPU(s) list:   0-23\n",
    "Vendor ID:               GenuineIntel\n",
    "  Model name:            Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\n",
    "    CPU family:          6\n",
    "    Model:               63\n",
    "    Thread(s) per core:  2\n",
    "    Core(s) per socket:  12\n",
    "    Socket(s):           1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03902619-8afb-4d6d-bd2a-18f084a93113",
   "metadata": {},
   "source": [
    "#### GPU:\n",
    "RTX 3060ti 8Gbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a1b93-b6af-49f4-b4bb-a059547f723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib para plotar o grafico.\n",
    "!poetry add matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaf088-18a8-478d-98c5-498f65a52d55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentação do Script: Plotando Ranking de Tempo de Execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde5646-0f00-4937-9142-4a6a3a3071ed",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script Python gera um gráfico de barras que mostra o ranking das funções em relação ao tempo de execução, ordenando-as do menor para o maior tempo.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura dos Dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/tempos_execucao.csv` que contém o nome da função e o tempo de execução (em segundos).\n",
    "* Cada linha do arquivo CSV deve conter:\n",
    "    - Nome da função (string).\n",
    "    - Tempo de execução no formato `X segundos` ou `Y minutos e Z segundos`.\n",
    "\n",
    "**2. Extração e Conversão:**\n",
    "\n",
    "* Extrai o nome da função e o tempo de execução de cada linha.\n",
    "* Converte o tempo de execução para segundos, considerando minutos e segundos.\n",
    "* Armazena os dados em uma lista de tuplas (`(nome_funcao, tempo_total)`).\n",
    "\n",
    "**3. Ordenação:**\n",
    "\n",
    "* Ordena a lista de tuplas pelo tempo de execução (tempo_total) em ordem crescente.\n",
    "\n",
    "**4. Plotagem do Gráfico:**\n",
    "\n",
    "* Cria um gráfico de barras com a biblioteca `matplotlib`.\n",
    "* As funções (ordenadas) são dispostas no eixo Y e o tempo de execução (em segundos) no eixo X.\n",
    "* As barras são coloridas em azul celeste.\n",
    "* O título do gráfico indica \"Tempo de Execução das Funções (Ordenado)\".\n",
    "* O eixo X é rotulado como \"Tempo de Execução (segundos)\" e o eixo Y como \"Função\".\n",
    "* Uma grade é adicionada ao eixo X para facilitar a leitura.\n",
    "* O tempo de execução de cada função é exibido acima da barra respectiva.\n",
    "\n",
    "**5. Salva o Gráfico:**\n",
    "\n",
    "* Salva o gráfico para visualização dos resultados na pasta img/ranking.png.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* O arquivo `data/tempos_execucao.csv` deve existir no caminho especificado.\n",
    " \n",
    "**Exemplo de Uso:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python plot_ranking.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fea12fb-0f2e-4e17-a6b2-f9be973fa004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráfico salvo com sucesso na pasta:img/ranking.png\n"
     ]
    }
   ],
   "source": [
    "!python src/plot_ranking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43440cbb-fb75-4aed-ac86-07ba8a6fe5ce",
   "metadata": {},
   "source": [
    "### Script para plotar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8841ca81-aeae-4ab6-866f-0651b6526b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/plot_ranking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/plot_ranking.py\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ranking_tempo_execucao(arquivo_csv):\n",
    "    # Leitura dos dados do arquivo CSV\n",
    "    dados = []\n",
    "\n",
    "    with open(arquivo_csv, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            nome_funcao = row[0]\n",
    "            tempo = row[1].split()  # Separa minutos e segundos\n",
    "            if len(tempo) == 2 and tempo[1] == 'segundos':  # Se houver apenas segundos\n",
    "                tempo_total = float(tempo[0])\n",
    "            elif len(tempo) == 4:  # Se houver minutos e segundos\n",
    "                minutos = float(tempo[0])\n",
    "                segundos = float(tempo[2])\n",
    "                tempo_total = minutos * 60 + segundos  # Converte minutos para segundos\n",
    "            else:\n",
    "                raise ValueError(f'Formato de tempo inválido: {row[1]}')\n",
    "            dados.append((nome_funcao, tempo_total))\n",
    "\n",
    "    # Ordena os dados pelo tempo de execução\n",
    "    dados_ordenados = sorted(dados, key=lambda x: x[1])\n",
    "\n",
    "    # Extrai os nomes das funções e os tempos de execução ordenados\n",
    "    nomes_funcoes_ordenados = [item[0] for item in dados_ordenados]\n",
    "    tempos_execucao_ordenados = [item[1] for item in dados_ordenados]\n",
    "\n",
    "    # Plotagem do gráfico\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nomes_funcoes_ordenados, tempos_execucao_ordenados, color='skyblue')\n",
    "    plt.xlabel('Tempo de Execução (segundos)')\n",
    "    plt.ylabel('Função')\n",
    "    plt.title('Tempo de Execução das Funções (Ordenado)')\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adicionando os valores nas barras\n",
    "    for i, valor in enumerate(tempos_execucao_ordenados):\n",
    "        plt.text(valor, i, f'{valor:.2f} s', va='center')\n",
    "\n",
    "    # Exibindo o gráfico\n",
    "    # plt.show()\n",
    "    plt.savefig('./img/ranking.png')\n",
    "    print('Gráfico salvo com sucesso na pasta:img/ranking.png')\n",
    "\n",
    "# Exemplo de uso da função\n",
    "if __name__ == \"__main__\":\n",
    "    arquivo_csv = 'data/tempos_execucao.csv'\n",
    "    plot_ranking_tempo_execucao(arquivo_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labpy3.10.13",
   "language": "python",
   "name": "labpy3.10.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
