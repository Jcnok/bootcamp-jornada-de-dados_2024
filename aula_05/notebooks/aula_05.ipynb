{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180d6e20-eb17-42b4-a459-0d7fa37ce1be",
   "metadata": {},
   "source": [
    "# Projeto 01 - Um Bilhão de Linhas: Desafio de Processamento de Dados com Python\n",
    "\n",
    "![imagem_01](./img/ranking.png)\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O objetivo deste projeto é demonstrar como processar eficientemente um arquivo de dados massivo contendo 1 bilhão de linhas (~14GB), especificamente para calcular estatísticas (Incluindo agregação e ordenação que são operações pesadas) utilizando Python. \n",
    "\n",
    "Este desafio foi inspirado no [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java, foi adaptado para o [bootcamp da jornada de dados 2024](https://www.jornadadedados2024.com.br/workshops)\n",
    "\n",
    "O arquivo de dados consiste em medições de temperatura de várias cidades(dados ficticios). Cada registro segue o formato `<string: nome da cidade>;<double: temperatura>`, com a temperatura sendo apresentada com precisão de uma casa decimal.\n",
    "\n",
    "Aqui estão dez linhas de exemplo do arquivo:\n",
    "\n",
    "```\n",
    "Hamburg;12.0\n",
    "Bulawayo;8.9\n",
    "Palembang;38.8\n",
    "St. Johns;15.2\n",
    "Cracow;12.6\n",
    "Bridgetown;26.9\n",
    "Istanbul;6.2\n",
    "Roseau;34.4\n",
    "Conakry;31.2\n",
    "Istanbul;23.0\n",
    "```\n",
    "\n",
    "O desafio é desenvolver um programa Python capaz de ler esse arquivo e calcular a temperatura mínima, média e máxima para cada cidade, exibindo os resultados em uma tabela ordenada por nome da Cidade.\n",
    "\n",
    "| city      | min_temperature | mean_temperature | max_temperature |\n",
    "|--------------|-----------------|------------------|-----------------|\n",
    "| Abha         | -31.1           | 18.0             | 66.5            |\n",
    "| Abidjan      | -25.9           | 26.0             | 74.6            |\n",
    "| Abéché       | -19.8           | 29.4             | 79.9            |\n",
    "| Accra        | -24.8           | 26.4             | 76.3            |\n",
    "| Addis Ababa  | -31.8           | 16.0             | 63.9            |\n",
    "| ...          | ...             | ...              | ...             |\n",
    "| Zagreb       | -39.2           | 10.7             | 58.1            |\n",
    "| Zanzibar City| -26.5           | 26.0             | 75.2            |\n",
    "| Zürich       | -42.0           | 9.3              | 63.6            |\n",
    "| Ürümqi       | -42.1           | 7.4              | 56.7            |\n",
    "| İzmir        | -34.4           | 17.9             | 67.9            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea9c65-126b-402c-8004-0e250a6a5e55",
   "metadata": {},
   "source": [
    "## Sobre:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe1e3a-8b1d-45d2-86fd-1b42b901a804",
   "metadata": {},
   "source": [
    "Abaixo um índice como todas as etapas para resolução do projeto, desde informações sobre as configurações desktop, sobre a memória, ssd, como gerar os dados, dependências e o resultado do tempo de execução de cada script utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d20fa0-eb4a-4d7d-9e89-d83facdf96df",
   "metadata": {},
   "source": [
    "## índice\n",
    "\n",
    "<a id=\"voltar\"></a>\n",
    "\n",
    "**[Como Replicar esse projeto](#ancora13)**\n",
    "1.  **[Decoradores](#ancora01)**\n",
    "2.  **[Scripts para gerar os dados.](#ancora02)**\n",
    "3.  **[Pandas  - min, max e mean em 1 bilhão de linhas](#ancora03)**\n",
    "4.  **[Polars  - min, max e mean em 1 bilhão de linhas](#ancora04)**\n",
    "5.  **[Duckdb  - min, max e mean em 1 bilhão de linhas](#ancora05)**\n",
    "6.  **[Dask    - min, max e mean em 1 bilhão de linhas](#ancora06)**\n",
    "7.  **[Pyspark - min, max e mean em 1 bilhão de linhas](#ancora07)**\n",
    "8.  **[Vaex    - min, max e mean em 1 bilhão de linhas](#ancora08)**\n",
    "9.  **[Cudf com pandas via GPU - min, max e mean em 1 bilhão de linhas](#ancora09)**\n",
    "10. **[Dask_cudf com GPU - min, max e mean em 1 bilhão de linhas](#ancora10)**\n",
    "11. **[Gerando um Gráfico dos Resultados](#ancora11)**\n",
    "12. **[Conclusão](#ancora12)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61922343-acac-483a-bebc-aba9072321f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora13\"></a>\n",
    "## Como Replicar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fb084-994f-4c07-a2f9-af85ed62bcfa",
   "metadata": {},
   "source": [
    "#### Pré-requisitos:\n",
    "\n",
    "* **Pyenv**: É usado para gerenciar versões do Python. [Instruções de instalação do Pyenv](https://github.com/pyenv/pyenv#installation).\n",
    "\n",
    "* **Poetry**: Este projeto utiliza Poetry para gerenciamento de dependências. [Instruções de instalação do Poetry](https://python-poetry.org/docs/#installation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854530f3-10f3-49cc-ade0-dbf32e7ead7d",
   "metadata": {},
   "source": [
    "#### Siga os passos abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef6f92-74f4-4c3a-b7b5-36967b40ccd1",
   "metadata": {},
   "source": [
    "- Crie uma pasta:\n",
    "\n",
    "```bash\n",
    "$ mkir meus_projetos\n",
    "```\n",
    "\n",
    "- Acesse a pasta criada:\n",
    "\n",
    "```bash\n",
    "$ cd meus_projetos\n",
    "```\n",
    "\n",
    "- Clone o repositório\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/Jcnok/bootcamp-jornada-de-dados_2024.git\n",
    "```\n",
    "\n",
    "- Acesse a pasta do repositório que foi clonado:\n",
    "\n",
    "```bash\n",
    "$ cd bootcamp-jornada-de-dados_2024/\n",
    "```\n",
    "\n",
    "- Verifique e se necessário configure a versão do python 3.10.13 para esta pasta:\n",
    "\n",
    "```bash\n",
    "$ pyenv local # para conferir\n",
    "$ pyenv local 3.10.3 #para configurar se preciso.\n",
    "```\n",
    "\n",
    "- Instale todas as dependências com poetry:\n",
    "\n",
    "```bash\n",
    "$ poetry install\n",
    "```\n",
    "\n",
    "- Acesse a pasta aula_05:\n",
    "\n",
    "```bash\n",
    "$ cd aula_05\n",
    "```\n",
    "\n",
    "- delete o arquivo ‘/data/tempos_execucao.csv’ para que possa ser criado um novo arquivo de tempo de execução:\n",
    "\n",
    "```bash\n",
    "$ rm data/tempos_execucao.csv\n",
    "```\n",
    "\n",
    "- Ative o ambiente virtual que foi criado com o poetry:\n",
    "\n",
    "```bash\n",
    "$ poetry shell\n",
    "```\n",
    "\n",
    "- Crie o arquivo com 1 bilhão de linhas:\n",
    "\n",
    "obs: o arquivo será criado na pasta `data\\measurements.txt` vai ser necessário 15gb de espaço e dependendo da máquina pode demorar, na minha máquina foram cerca de 15 minutos.\n",
    "\n",
    "```bash\n",
    "$ python src/data_generate.py\n",
    "```\n",
    "\n",
    "- Agora basta executar os scripts conforme abaixo:\n",
    "\n",
    "```bash\n",
    "$ python src/polars_df.py      # +- 40 segundos\n",
    "$ python src/pandas_df.py      # +- 10 minutos\n",
    "$ python src/vaex_df.py        # o script não suportou 1 bilhão\n",
    "$ python src/pyspark_df.py     # +- 4 minutos\n",
    "$ python src/duckdb_df.py      # +- 35 segundos**\n",
    "$ python src/dask_df.py        # +- 7 minutos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000e66e-600c-4bbe-bc68-dad250afe39b",
   "metadata": {},
   "source": [
    "#### Caso possua uma GPU compatível siga os passos abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37814681-3ce9-457e-b1df-4665e8585bde",
   "metadata": {},
   "source": [
    "* **[Cudf com pandas via GPU](#ancora09)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130acd9-f26b-411b-aea0-e686bb1cf115",
   "metadata": {},
   "source": [
    "* **[Dask_cudf com GPU](#ancora10)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7325031-a722-470c-a200-902168cc54f8",
   "metadata": {},
   "source": [
    "#### Após as devidas verificações/instalações:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d91747-9922-42e2-be74-35ba3f0a230f",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ python src/cudf_pandas_df.py # +- 14 minutos\n",
    "$ python src/dask_cudf_1024.py # +- 2 minutos\n",
    "$ python src/dask_cudf_256.py  # +- 1 minuto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323457a0-fb58-4c6e-9780-9aca3b1fdc31",
   "metadata": {},
   "source": [
    "#### Você ainda pode executar todo o passo a passo utilizando o jupyter notebook:\n",
    "```bash\n",
    "$ cd notebooks/\n",
    "$ jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b94323-50ca-4fce-86ff-1e1f14618d63",
   "metadata": {},
   "source": [
    "* **Com o jupyter lab aberto abra a 'aula_05.ipynb' e execute as celulas na sequência...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b150520-d10e-4437-9909-2180561c58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d98c0-4aa7-4070-85d2-01d5847d14c1",
   "metadata": {},
   "source": [
    "* **Selecionando a pasta Aula_05 para iniciar e criar os scritps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b431b0-5232-4c2e-afae-891b76be7901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conferir e selecionar a pasta raiz do projeto.\n",
    "os.getcwd()\n",
    "os.chdir('/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06072a-fd42-4806-b8f3-c8e57728817d",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7b87b-9ff0-4e17-bac4-ec9dc191add3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora01\"></a>\n",
    "## Decoradores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db8df0-beb2-4047-ae79-02521b633225",
   "metadata": {},
   "source": [
    "**O decorator timer serve para medir o tempo de execução de uma função e registrar o resultado em um arquivo CSV. Ele também imprime o tempo de execução no console.**\n",
    "\n",
    "**Porquê usar decorators?**\n",
    "* Nesse exemplo precisamos medir o tempo de cada uma das funções que serão criadas para determinar a melhor performance, dessa forma podemos criar uma função mais limpa e organizada onde iremos chamar o decorator para exibir e registrar as informações do tempo de execução da cada uma das funções, isso evita a duplicidade de código.\n",
    "\n",
    "* Funcionalidades:\n",
    "\n",
    "    - Mede o tempo de execução da função decorada.\n",
    "    - Formata o tempo em horas, minutos e segundos.\n",
    "    - Registra o nome da função e o tempo de execução em um arquivo CSV chamado tempos_execucao.csv.\n",
    "    - Imprime o nome da função e o tempo de execução no console.\n",
    "* Vantagens de usar decorators:\n",
    "\n",
    "    - Códigos mais concisos e reutilizáveis.\n",
    "    - Evita a duplicação de código para medição de tempo.\n",
    "    - Facilita a comparação do tempo de execução de diferentes funções.\n",
    "    - Permite a criação de logs de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f552d-c24f-4034-9248-f15c2044c199",
   "metadata": {},
   "source": [
    "* **Decorator para salvar os resultado em um csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7cc2e71-787d-4389-b0cb-ee625c8bccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/utils/decorators.py \n",
    "#decorator para registrar o tempo de execução da função em um .csv\n",
    "import time\n",
    "import csv\n",
    "import threading\n",
    "\n",
    "\n",
    "def timer_to_csv(func):\n",
    "    def format_time(segundos: int):\n",
    "        \"\"\"\n",
    "        Formata os milisegundos em hora:minuto:segundo\n",
    "        \"\"\"\n",
    "        if segundos < 60:\n",
    "            return f\"{segundos:.3f} segundos\"\n",
    "        elif segundos < 3600:\n",
    "            minutos, segundos = divmod(segundos, 60)\n",
    "            return f\"{int(minutos)} minutos {int(segundos)} segundos\"\n",
    "        else:\n",
    "            horas, remainder = divmod(segundos, 3600)\n",
    "            minutos, segundos = divmod(remainder, 60)\n",
    "            if minutos == 0:\n",
    "                return f\"{int(horas)} horas {int(segundos)} segundos\"\n",
    "            else:\n",
    "                return f\"{int(horas)} horas {int(minutos)} minutos {int(segundos)} segundos\"\n",
    "\n",
    "    def print_elapsed_time(start_time, finished_flag):\n",
    "        while not finished_flag.is_set():\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Tempo decorrido: {format_time(elapsed_time)}\", end=\"\\r\")\n",
    "            time.sleep(1)  # Atualiza a cada segundo\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Inicializa um sinalizador booleano para indicar se a função terminou\n",
    "        finished_flag = threading.Event()\n",
    "\n",
    "        # Iniciar uma thread para imprimir o tempo decorrido em tempo real\n",
    "        thread = threading.Thread(target=print_elapsed_time, args=(start_time, finished_flag))\n",
    "        thread.start()\n",
    "\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Tempo total de execução: {format_time(elapsed_time)}\")\n",
    "\n",
    "        # Sinaliza que a função terminou\n",
    "        finished_flag.set()\n",
    "\n",
    "        # Salvando o nome da função e o tempo de execução em um arquivo CSV\n",
    "        with open('data/tempos_execucao.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([func.__name__, format_time(elapsed_time)])\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b229b-a9ad-40e5-8df7-c006e6c3d590",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91068054-2252-45e8-a9d1-12303e8ce8ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora02\"></a>\n",
    "## Scripts para gerar os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf73386-2b9e-4393-a9bf-06b1e88191d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script utilizando Pandas - not suport 1billion rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a77b5ee-6e1f-4e37-892a-bfa6e2cea78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib Pandas;\n",
    "!poetry add pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dda09dcd-7484-423a-9850-92bd201611f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a quantidade de linhas desejadas:  10000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 s, sys: 1.83 s, total: 28.8 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_data(num_rows):\n",
    "    \"\"\"\n",
    "    Gera dados aleatórios de temperatura para cidades e salva em um arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): Número de linhas desejadas para o DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Amostrar aleatoriamente os nomes das cidades existentes\n",
    "    city_names = pd.read_csv('data/weather_stations.csv', sep=';', header=None,\n",
    "                             skiprows=2, usecols=[0]).sample(num_rows, replace=True).iloc[:, 0].tolist()\n",
    "\n",
    "    # Gerar temperaturas aleatórias\n",
    "    temperatures = [round(random.uniform(-89.2, 56.7), 1) for _ in range(num_rows)]\n",
    "\n",
    "    # Criar um DataFrame pandas com os dados gerados\n",
    "    df = pd.DataFrame({'city': city_names, 'temperature': temperatures})\n",
    "\n",
    "    # Salvar o DataFrame em um arquivo CSV\n",
    "    df.to_csv('data/measurements_pandas.txt', sep=';', header=False, index=False)\n",
    "\n",
    "    # salvar o valor da quantidade de linhas solicitada\n",
    "    with open('data/num_rows.txt', 'w') as file:\n",
    "        # Escrever o valor no arquivo\n",
    "        file.write(str(num_rows))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_rows = int(input(\"Digite a quantidade de linhas desejadas: \"))\n",
    "    generate_data(num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f3d22-fc17-47ef-be53-1d1a345128c4",
   "metadata": {},
   "source": [
    "* **Obs:**\n",
    "* Foram gerados 10 milhões de linhas para nossos primeiros testes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c920d-fd60-4f2d-a2d4-3191a0c0b6ea",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274703a-146f-472d-a0fa-3fadebf510bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scrip python para gerar dados aleatórios\n",
    "* **O script foi retirado do desafio [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java.**\n",
    "* **Algumas alterações foram realizadas, como por exemplo a temp máxima e mínima foram alterados para valores históricos reais.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ff1f98-314d-4e04-8e10-28da8764ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/data_generate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/data_generate.py\n",
    "# Script para gerar 1 bilhão de linhas com dados aleatórios.\n",
    "# Based on https://github.com/gunnarmorling/1brc/blob/main/src/main/java/dev/morling/onebrc/Createtempments.java\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def check_args(file_args):\n",
    "    \"\"\"\n",
    "    Sanity checks out input and prints out usage if input is not na positive integer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(file_args) != 2 or int(file_args[1]) <= 0:\n",
    "            raise Exception()\n",
    "    except:\n",
    "        print(\"Usage:  create_tempments.sh <positive integer number of records to create>\")\n",
    "        print(\"        You can use underscore notation for large number of records.\")\n",
    "        print(\"        For example:  1_000_000_000 for one billion\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def build_weather_city_name_list():\n",
    "    \"\"\"\n",
    "    Grabs the weather city names from example data provided in repo and dedups\n",
    "    \"\"\"    \n",
    "    city_names = []\n",
    "    with open('../data/weather_stations.csv', 'r') as file: \n",
    "\n",
    "        file_contents = file.read()\n",
    "    for city in file_contents.splitlines():\n",
    "        if \"#\" in city:\n",
    "            next\n",
    "        else:\n",
    "            city_names.append(city.split(';')[0])\n",
    "    return list(set(city_names))\n",
    "\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    Convert bytes to a human-readable format (e.g., KiB, MiB, GiB)\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KiB', 'MiB', 'GiB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def format_elapsed_time(seconds):\n",
    "    \"\"\"\n",
    "    Format elapsed time in a human-readable format\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.3f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes, seconds = divmod(seconds, 60)\n",
    "        return f\"{int(minutes)} minutes {int(seconds)} seconds\"\n",
    "    else:\n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        if minutes == 0:\n",
    "            return f\"{int(hours)} hours {int(seconds)} seconds\"\n",
    "        else:\n",
    "            return f\"{int(hours)} hours {int(minutes)} minutes {int(seconds)} seconds\"\n",
    "\n",
    "\n",
    "def estimate_file_size(weather_city_names, num_rows_to_create):\n",
    "    \"\"\"\n",
    "    Tries to estimate how large a file the test data will be\n",
    "    \"\"\"\n",
    "    total_name_bytes = sum(len(s.encode(\"utf-8\")) for s in weather_city_names)\n",
    "    avg_name_bytes = total_name_bytes / float(len(weather_city_names))\n",
    "\n",
    "    # avg_temp_bytes = sum(len(str(n / 10.0)) for n in range(-999, 1000)) / 1999\n",
    "    avg_temp_bytes = 4.400200100050025\n",
    "\n",
    "    # add 2 for separator and newline\n",
    "    avg_line_length = avg_name_bytes + avg_temp_bytes + 2\n",
    "\n",
    "    human_file_size = convert_bytes(num_rows_to_create * avg_line_length)\n",
    "\n",
    "    return f\"Estimated max file size is:  {human_file_size}.\"\n",
    "\n",
    "\n",
    "def build_test_data(weather_city_names, num_rows_to_create):\n",
    "    \"\"\"\n",
    "    Generates and writes to file the requested length of test data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    coldest_temp = -89.2\n",
    "    hottest_temp = 56.7\n",
    "    city_names_10k_max = random.choices(weather_city_names, k=10_000)\n",
    "    batch_size = 10000 # instead of writing line by line to file, process a batch of citys and put it to disk\n",
    "    chunks = num_rows_to_create // batch_size\n",
    "    print('Building test data...')\n",
    "\n",
    "    try:\n",
    "        with open(\"../data/measurements.txt\", 'w') as file:\n",
    "            progress = 0\n",
    "            for chunk in range(chunks):\n",
    "                \n",
    "                batch = random.choices(city_names_10k_max, k=batch_size)\n",
    "                prepped_deviated_batch = '\\n'.join([f\"{city};{random.uniform(coldest_temp, hottest_temp):.1f}\" for city in batch]) # :.1f should quicker than round on a large scale, because round utilizes mathematical operation\n",
    "                file.write(prepped_deviated_batch + '\\n')\n",
    "\n",
    "                \n",
    "                # Update progress bar every 1%\n",
    "                if (chunk + 1) * 100 // chunks != progress:\n",
    "                    progress = (chunk + 1) * 100 // chunks\n",
    "                    bars = '=' * (progress // 2)\n",
    "                    sys.stdout.write(f\"\\r[{bars:<50}] {progress}%\")\n",
    "                    sys.stdout.flush()\n",
    "        sys.stdout.write('\\n')\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong. Printing error info and exiting...\")\n",
    "        print(e)\n",
    "        exit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    file_size = os.path.getsize(\"../data/measurements.txt\")\n",
    "    human_file_size = convert_bytes(file_size)\n",
    " \n",
    "    print(\"Test data successfully written to data/measurements.txt\")\n",
    "    print(f\"Actual file size:  {human_file_size}\")\n",
    "    print(f\"Elapsed time: {format_elapsed_time(elapsed_time)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    main program function\n",
    "    \"\"\"\n",
    "    num_rows_to_create = 1_000_000_000\n",
    "    weather_city_names = []\n",
    "    weather_city_names = build_weather_city_name_list()\n",
    "    print(estimate_file_size(weather_city_names, num_rows_to_create))\n",
    "    build_test_data(weather_city_names, num_rows_to_create)\n",
    "    print(\"Test data build complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fada2-9078-4ba4-b50e-f9b1e89faa88",
   "metadata": {},
   "source": [
    "#### Como executar o script pelo terminal:\n",
    " \n",
    "\n",
    "$`python src/utils/data_generate.py`#para criar os dados na pasta 'data/mesurements.txt'\n",
    "\n",
    "* **obs**: O script depende do arquivo 'data/weather_stations.csv' para gerar os dados das cidades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad06450-e001-4bd8-a2a3-0bc2e9547807",
   "metadata": {},
   "source": [
    "* **Agora com os dados de 1 bilhao gerados vamos ver quem se sai melhor para as configuracoes da minha maquina**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1469fd3-e5d9-4bd8-8a52-934abb1c06f2",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c14b73-eeef-4545-a1aa-11f872ae2d6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora03\"></a>\n",
    "## Pandas - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81ac2551-f634-411a-bafe-70ceb59bdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a lib pandas.\n",
    "!poetry add pandas tabulate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38742de5-a0ce-4b7d-a260-08a3f2bc04d7",
   "metadata": {},
   "source": [
    "### Documentação do Script Python: Calculando Mínimo, Máximo e Média para um Bilhão de Linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034f5c-e3f2-419a-b89f-c455e50ab136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Este script calcula a temperatura mínima, máxima e média para cada cidade em um grande conjunto de dados contendo um bilhão de linhas. Ele utiliza as bibliotecas Pandas, multiprocessing e tqdm.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Leitura de dados:** \n",
    "    * Lê dados de um arquivo CSV (`data/measurements.txt`) com colunas nomeadas `cidade` e `temp`.\n",
    "    * Emprega fragmentação para processar os dados em partes menores e gerenciáveis.\n",
    "\n",
    "2. **Processamento paralelo:**\n",
    "    * Aproveita o multiprocessing para distribuir a carga de trabalho entre os núcleos de CPU disponíveis.\n",
    "    * Cada fragmento é processado independentemente pela função `process_chunk`.\n",
    "\n",
    "3. **Agregação:**\n",
    "    * Dentro de cada fragmento, a função `process_chunk` usa Pandas para:\n",
    "        * Agrupar dados por `cidade`.\n",
    "        * Calcular o `max`, `min` e `mean` da coluna `temp`.\n",
    "        * Redefinir o índice para obter um DataFrame limpo.\n",
    "\n",
    "4. **Agregação de resultados:**\n",
    "    * Resultados de fragmentos individuais são concatenados em um único DataFrame.\n",
    "    * Uma agregação final é realizada para calcular o `max`, `min` e `mean` geral para cada cidade.\n",
    "    * O DataFrame final é classificado por `cidade`.\n",
    "\n",
    "5. **Visualização do progresso:**\n",
    "    * A biblioteca tqdm fornece uma barra de progresso para visualizar o processamento dos fragmentos.\n",
    "\n",
    "**Principais características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** A fragmentação e o multiprocessing permitem lidar com dados massivos com eficiência.\n",
    "* **Desempenho otimizado:** A utilização de vários núcleos reduz significativamente o tempo de processamento.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de que a variável `filename` aponte para o local correto do arquivo de dados `data/measurements.txt`.\n",
    "* Ajuste `chunksize` dependendo da memória do seu sistema e dos recursos de processamento.\n",
    "\n",
    "**Documentação**: [Pandas](https://pandas.pydata.org/docs/)\n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/pandas_df.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fa4443-164f-4349-834b-a82a6827efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processamento do arquivo.\n",
      "Tempo total de execução: 9 minutos 45 segundos\n",
      "               city   max   min   mean\n",
      "0          A Coruña  56.7 -89.2 -16.32\n",
      "1          Aabenraa  56.7 -89.2 -16.47\n",
      "2             Aalen  56.7 -89.2 -16.37\n",
      "3          Aarschot  56.7 -89.2 -16.03\n",
      "4        Aartselaar  56.7 -89.2 -16.34\n",
      "...             ...   ...   ...    ...\n",
      "8898         ‘Aqrah  56.7 -89.2 -16.31\n",
      "8899  ‘Ayn al ‘Arab  56.7 -89.2 -16.09\n",
      "8900          ‘Ibrī  56.7 -89.2 -16.42\n",
      "8901     ’Aïn Arnat  56.7 -89.2 -16.31\n",
      "8902      ’Aïn Roua  56.7 -89.2 -16.15\n",
      "\n",
      "[8903 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "!python src/pandas_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ecb0-8a95-4553-a846-5915a66c9748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script Pandas_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c2f098-c6bd-4b4a-85d5-47ea486ed63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/pandas_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pandas_df.py\n",
    "#pandas => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "import pandas as pd \n",
    "from multiprocessing import Pool, cpu_count\n",
    "from utils.config import PATH, NUM_ROWS\n",
    "from utils.decorators import timer_to_csv  \n",
    "\n",
    "CONCURRENCY = cpu_count()\n",
    "\n",
    "def get_total_lines(num_rows_path):\n",
    "    with open(num_rows_path, 'r') as f:\n",
    "        total_lines = f.read()\n",
    "    return int(total_lines)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Agrega os dados dentro do chunk usando Pandas\n",
    "    aggregated = chunk.groupby('city')['temp'].agg(['max','min','mean']).reset_index()\n",
    "    return aggregated\n",
    "\n",
    "@timer_to_csv  # Aplica o decorador\n",
    "def pandas_df(filename, num_rows_path):\n",
    "    total_linhas = get_total_lines(num_rows_path)\n",
    "    chunksize = total_linhas // 200 + (1 if total_linhas % 10 else 0)  # Define o tamanho do chunk\n",
    "    with pd.read_csv(filename, sep=';', header=None, names=['city', 'temp'], chunksize=chunksize) as reader:\n",
    "        with Pool(CONCURRENCY) as pool:\n",
    "            results = []\n",
    "            for result in pool.imap(process_chunk, reader):\n",
    "                results.append(result)\n",
    "\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    final_aggregated_df = final_df.groupby('city').agg({\n",
    "        'max': 'max',\n",
    "        'min': 'min',\n",
    "        'mean': 'mean'\n",
    "        \n",
    "    }).reset_index().round(2).sort_values('city')\n",
    "\n",
    "    return final_aggregated_df\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    print(\"Iniciando o processamento do arquivo.\")\n",
    "    num_rows_path= NUM_ROWS\n",
    "    filename = PATH     \n",
    "    df = pandas_df(filename, num_rows_path)\n",
    "    #print(tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "    print(df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71707b70-8127-4d5d-905b-7221345730c2",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be5f4c-6870-4f2b-8403-3e4751da0eb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora04\"></a>\n",
    "## Polars - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e622fcb8-9f8f-4b34-bf90-e491ccf0bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a lib polars\n",
    "!poetry add polars -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f49575-3f35-4543-9591-b0059095b415",
   "metadata": {},
   "source": [
    "### Documentação do Script `polars_df.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfcd41-b9cb-4d9b-b15d-0c78c2f742e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Este script Python utiliza a biblioteca Polars para calcular de forma eficiente a temperatura mínima, máxima e média para cada cidade em um conjunto de dados grande (por exemplo, um bilhão de linhas). \n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Importações:**\n",
    "    * Importa a função `timer_to_csv` do módulo `utils.decorators`. \n",
    "    * Importa a biblioteca `polars` como `pl`.\n",
    "\n",
    "2. **Configuração do Polars:**\n",
    "    * Define o tamanho do chunk para processamento de streaming como 100.000 linhas usando `pl.Config.set_streaming_chunk_size(100000)`. Isso otimiza o uso de memória ao lidar com grandes conjuntos de dados.\n",
    "\n",
    "3. **Função `polars_df()`:**\n",
    "    * Esta função é decorada com `@timer_to_csv`, que mede o tempo de execução e salva os resultados em um arquivo CSV.\n",
    "    * Lê o arquivo CSV `data/measurements.txt` com as seguintes características:\n",
    "        * Separador: `;`\n",
    "        * Sem cabeçalho (`has_header=False`)\n",
    "        * Esquema de colunas: `{\"city\": pl.String, \"temp\": pl.Float64}`\n",
    "    * Agrupa os dados por `city`.\n",
    "    * Calcula as seguintes estatísticas para cada cidade:\n",
    "        * Temperatura máxima (`max_temp`)\n",
    "        * Temperatura mínima (`min_temp`)\n",
    "        * Temperatura média (`mean_temp`)\n",
    "    * Ordena os resultados por `city`.\n",
    "    * Coleta os resultados em um DataFrame Polars usando `collect(streaming=True)` para processamento eficiente de grandes conjuntos de dados.\n",
    "    * Retorna o DataFrame Polars resultante.\n",
    "\n",
    "4. **Execução principal:**\n",
    "    * Se o script for executado diretamente (e não importado como um módulo), a função `polars_df()` é chamada e o DataFrame resultante é impresso no console.\n",
    "\n",
    "**Principais características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** O Polars é otimizado para lidar com grandes conjuntos de dados, utilizando processamento em chunks e técnicas eficientes de gerenciamento de memória.\n",
    "* **Sintaxe expressiva:** A API do Polars permite realizar a análise de dados de forma concisa e legível.\n",
    "* **Desempenho:** O Polars é frequentemente mais rápido que o Pandas, especialmente em grandes conjuntos de dados.\n",
    "\n",
    "**Documentação:** [Polars](https://docs.pola.rs/#philosophy)\n",
    "\n",
    "Este script demonstra como o Polars pode ser usado para processar grandes conjuntos de dados de forma eficiente e calcular estatísticas básicas. \n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/polars_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef625854-08e9-4ab1-8dbc-1be8ab82df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução: 44.287 segundos\n",
      "shape: (8_903, 4)\n",
      "┌───────────────┬──────────┬──────────┬────────────┐\n",
      "│ city          ┆ max_temp ┆ min_temp ┆ mean_temp  │\n",
      "│ ---           ┆ ---      ┆ ---      ┆ ---        │\n",
      "│ str           ┆ f64      ┆ f64      ┆ f64        │\n",
      "╞═══════════════╪══════════╪══════════╪════════════╡\n",
      "│ A Coruña      ┆ 56.7     ┆ -89.2    ┆ -16.32501  │\n",
      "│ Aabenraa      ┆ 56.7     ┆ -89.2    ┆ -16.461449 │\n",
      "│ Aalen         ┆ 56.7     ┆ -89.2    ┆ -16.367746 │\n",
      "│ Aarschot      ┆ 56.7     ┆ -89.2    ┆ -16.028291 │\n",
      "│ Aartselaar    ┆ 56.7     ┆ -89.2    ┆ -16.336842 │\n",
      "│ …             ┆ …        ┆ …        ┆ …          │\n",
      "│ ‘Aqrah        ┆ 56.7     ┆ -89.2    ┆ -16.31633  │\n",
      "│ ‘Ayn al ‘Arab ┆ 56.7     ┆ -89.2    ┆ -16.092674 │\n",
      "│ ‘Ibrī         ┆ 56.7     ┆ -89.2    ┆ -16.423088 │\n",
      "│ ’Aïn Arnat    ┆ 56.7     ┆ -89.2    ┆ -16.309803 │\n",
      "│ ’Aïn Roua     ┆ 56.7     ┆ -89.2    ┆ -16.148495 │\n",
      "└───────────────┴──────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "!python src/polars_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfa647-57bc-4de3-9b26-3f25d2f43036",
   "metadata": {},
   "source": [
    "### Script Polars_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31de0d4-931d-4fd3-8353-373c41062cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/polars_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/polars_df.py\n",
    "# Polars => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH, NUM_ROWS\n",
    "import polars as pl\n",
    "\n",
    "def get_total_lines(num_rows_path):\n",
    "    with open(num_rows_path, 'r') as f:\n",
    "        total_lines = f.read()\n",
    "    return int(total_lines)\n",
    "\n",
    "@timer_to_csv\n",
    "def polars_df(filename, num_rows_path):\n",
    "    total_linhas = get_total_lines(num_rows_path)\n",
    "    chunksize = total_linhas // 200 + (1 if total_linhas % 10 else 0)  # Define o tamanho do chunk\n",
    "\n",
    "    pl.Config.set_streaming_chunk_size(chunksize)\n",
    "\n",
    "    # Leitura do arquivo CSV e definição do schema\n",
    "    return (pl.scan_csv(filename, separator=\";\", has_header=False,\n",
    "                        schema={\"city\": pl.String, \"temp\": pl.Float64})\n",
    "                        .group_by(\"city\").agg(\n",
    "                                                 max_temp=pl.col(\"temp\").max(),\n",
    "                                                 min_temp=pl.col(\"temp\").min(),\n",
    "                                                 mean_temp=pl.col(\"temp\").mean()\n",
    "                                                ).sort(\"city\").collect(streaming=True)\n",
    "           )\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    num_rows_path= NUM_ROWS\n",
    "    filename = PATH \n",
    "    df = polars_df(filename, num_rows_path)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf892a0b-d562-4ca3-b979-0d19daa9290e",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0fc93-52a7-4a33-9529-a3e8e78241c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora05\"></a>\n",
    "## Duckdb - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a4123a-14a4-4fd8-9336-c1a0dc1ab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add duckdb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7daa-a692-4d27-90f3-2c3535838cc5",
   "metadata": {},
   "source": [
    "### Documentação do Script `duckdb_df.py`\n",
    "\n",
    "Este script Python utiliza a biblioteca DuckDB para calcular de forma eficiente a temperatura mínima, máxima e média para cada cidade em um conjunto de dados grande (por exemplo, um bilhão de linhas). \n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "1. **Importações:**\n",
    "    * Importa a função `timer_to_csv` do módulo `utils.decorators`. \n",
    "    * Importa a biblioteca `duckdb`.\n",
    "\n",
    "2. **Função `duckdb_df()`:**\n",
    "    * Esta função é decorada com `@timer_to_csv`, que mede o tempo de execução e salva os resultados em um arquivo CSV.\n",
    "    * Executa uma consulta SQL no DuckDB para:\n",
    "        * Ler o arquivo CSV `data/measurements.txt` com as seguintes características:\n",
    "            * Detecção automática de tipo de dados desativada (`AUTO_DETECT=FALSE`)\n",
    "            * Separador: `;`\n",
    "            * Colunas: `city` (VARCHAR) e `temp` (DECIMAL)\n",
    "        * Agrupar os dados por `city`.\n",
    "        * Calcular as seguintes estatísticas para cada cidade:\n",
    "            * Temperatura máxima (`max_temp`)\n",
    "            * Temperatura mínima (`min_temp`)\n",
    "            * Temperatura média (`mean_temp`) - convertida para DECIMAL\n",
    "        * Ordenar os resultados por `city`.\n",
    "    * Exibe os resultados da consulta no console.\n",
    "\n",
    "3. **Execução principal:**\n",
    "    * Se o script for executado diretamente (e não importado como um módulo), a função `duckdb_df()` é chamada.\n",
    "\n",
    "**Documentação:** [Duckdb](https://duckdb.org/docs/)\n",
    "\n",
    "**Como executar o script:**\n",
    "\n",
    "1. **Terminal Bash digite:** `cd aula_05`\n",
    "2. **Execute o script:** Digite o seguinte comando e pressione Enter: \n",
    "```bash\n",
    "python src/duckdb_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367987fa-6708-45bb-8261-95af03d5fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city  max_temp  min_temp  mean_temp\n",
      "0          A Coruña      56.7     -89.2    -16.325\n",
      "1          Aabenraa      56.7     -89.2    -16.461\n",
      "2             Aalen      56.7     -89.2    -16.368\n",
      "3          Aarschot      56.7     -89.2    -16.028\n",
      "4        Aartselaar      56.7     -89.2    -16.337\n",
      "...             ...       ...       ...        ...\n",
      "8898         ‘Aqrah      56.7     -89.2    -16.316\n",
      "8899  ‘Ayn al ‘Arab      56.7     -89.2    -16.093\n",
      "8900          ‘Ibrī      56.7     -89.2    -16.423\n",
      "8901     ’Aïn Arnat      56.7     -89.2    -16.310\n",
      "8902      ’Aïn Roua      56.7     -89.2    -16.148\n",
      "\n",
      "[8903 rows x 4 columns]\n",
      "Tempo total de execução: 23.520 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/duckdb_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21464745-8551-4887-9327-de61861b0814",
   "metadata": {},
   "source": [
    "### Script Duck_db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d06b82-0eb9-4580-9d56-1588a191dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/duckdb_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/duckdb_df.py\n",
    "# Duckdb => script para caluclar min, max e mean.\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "import duckdb\n",
    "\n",
    "@timer_to_csv\n",
    "def duckdb_df(filename):\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    query = f\"\"\"\n",
    "            SELECT city,\n",
    "                MAX(temp) AS max_temp,\n",
    "                MIN(temp) AS min_temp,\n",
    "                CAST(AVG(temp) AS DECIMAL()) AS mean_temp                \n",
    "            FROM read_csv(\"{filename}\", AUTO_DETECT=FALSE, sep=';', columns={{'city':VARCHAR, 'temp': 'DECIMAL'}})\n",
    "            GROUP BY city\n",
    "            ORDER BY city\n",
    "        \"\"\"\n",
    "    print(conn.execute(query).df())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = PATH \n",
    "    duckdb_df(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7739a-61ed-4241-9169-556a97a2cc87",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a87d1-37ef-4def-960f-1e885b6008cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora06\"></a>\n",
    "## Dask - min, max e mean em 1 bilhão de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "99cd03e5-8f9e-4f2c-9397-91d7fc3dfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessário Instalar.\n",
    "!poetry add dask-expr -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c9d87-b589-4c65-aa5d-d67653581258",
   "metadata": {},
   "source": [
    "### Documentação do Script Dask:\n",
    "\n",
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados massivo contendo um bilhão de linhas. Ele utiliza as bibliotecas Dask para otimizar o processamento e fornecer uma experiência interativa.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê dados de um arquivo CSV (`data/measurements.txt`) com colunas nomeadas `city` e `temp`.\n",
    "* Emprega a biblioteca Dask para ler o arquivo em um DataFrame particionado, permitindo processamento eficiente em grandes conjuntos de dados.\n",
    "\n",
    "**2. Processamento paralelo:**\n",
    "\n",
    "* O Dask gerencia o particionamento e o processamento paralelo do DataFrame em diferentes threads ou processos.\n",
    "* Cada partição é processada independentemente pela função `process_chunk`.\n",
    "\n",
    "**3. Agregação:**\n",
    "\n",
    "* Dentro de cada partição, a função `process_chunk` usa Pandas para:\n",
    "    * Agrupar dados por `city`.\n",
    "    * Calcular o `max`, `min` e `mean` da coluna `temp`.\n",
    "    * Redefinir o índice para obter um DataFrame limpo.\n",
    "\n",
    "**4. Agregação de resultados:**\n",
    "\n",
    "* Os resultados de cada partição são combinados em um único DataFrame.\n",
    "* Uma agregação final é realizada para calcular o `max`, `min` e `mean` geral para cada cidade.\n",
    "* O DataFrame final é ordenado por `city`.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento eficiente de grandes conjuntos de dados:** O Dask permite lidar com dados massivos de forma eficiente e escalável.\n",
    "* **Desempenho otimizado:** O processamento paralelo em partições reduz significativamente o tempo de processamento.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de que a variável `filename` aponte para o local correto do arquivo de dados `data/measurements.txt`.\n",
    "* Ajuste o tamanho da partição (`chunksize`) de acordo com a memória do seu sistema e os recursos de processamento.\n",
    "* Se necessário, configure o Dask para usar um cluster de computação para aumentar ainda mais o desempenho.\n",
    "\n",
    "**Documentação:** [Dask](https://docs.dask.org/en/stable/)\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd aula_05`\n",
    "2. **Execute o script:** `python src/dask_df.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdcb3c5d-17b4-46dd-a2c0-cf61a1a8f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/__init__.py\", line 10, in _dask_expr_enabled\n",
      "    import dask_expr  # noqa: F401\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_expr/__init__.py\", line 3, in <module>\n",
      "    from dask_expr import _version, datasets\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_expr/datasets.py\", line 6, in <module>\n",
      "    from dask.dataframe.utils import pyarrow_strings_enabled\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/utils.py\", line 20, in <module>\n",
      "    from dask.dataframe import (  # noqa: F401 register pandas extension types\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/methods.py\", line 34, in <module>\n",
      "    from dask.dataframe.utils import is_dataframe_like, is_index_like, is_series_like\n",
      "ImportError: cannot import name 'is_dataframe_like' from partially initialized module 'dask.dataframe.utils' (most likely due to a circular import) (/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05/src/dask_df.py\", line 6, in <module>\n",
      "    import dask.dataframe as dd\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/__init__.py\", line 16, in <module>\n",
      "    if _dask_expr_enabled():\n",
      "  File \"/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask/dataframe/__init__.py\", line 12, in _dask_expr_enabled\n",
      "    raise ValueError(\"Must install dask-expr to activate query planning.\")\n",
      "ValueError: Must install dask-expr to activate query planning.\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117e34d-6386-4149-a5c9-bab7aca7331f",
   "metadata": {},
   "source": [
    "### Script Dask_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f02dcf6-7633-4b3a-b68e-4d7a9c50a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/dask_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_df.py\n",
    "# Dask => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "@timer_to_csv\n",
    "def dask_df(filename):    \n",
    "    # Ler o arquivo txt diretamente em um DataFrame Dask\n",
    "    df = dd.read_csv(filename, delimiter=';', \n",
    "                     header=None, names=['city', 'temp'])\n",
    "    # min, max, e mean pela cidade ordenado pelo index\n",
    "    return print(df.groupby('city').\n",
    "                   agg({'temp': ['max','min','mean']}).\n",
    "                   compute().round(2).sort_index())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = PATH\n",
    "    dask_df(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0aeceb-0913-4ec0-ae4c-5203c00e3257",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5a641-c514-4c94-a368-64424d857d9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora07\"></a>\n",
    "## Pyspark - min, max e mean em 1 bilhão de linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f73b4-0590-48b2-9bce-0409c9aea24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalação da lib\n",
    "!poetry add pyspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200eba8-d287-4b4e-b33e-4bf3b177877d",
   "metadata": {},
   "source": [
    "### Documentação do Script PySpark:\n",
    "\n",
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados massivo contendo um bilhão de linhas utilizando a biblioteca PySpark.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Inicialização da sessão Spark:**\n",
    "\n",
    "* Cria um objeto `SparkSession` para interagir com o cluster Spark.\n",
    "* Define o nome da aplicação como \"Temperature Analysis\".\n",
    "\n",
    "**2. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements.txt` diretamente em um DataFrame Spark.\n",
    "* Especifica que o arquivo não possui cabeçalho e usa ponto-e-vírgula como delimitador.\n",
    "* Define os nomes das colunas como \"City\" e \"Temperature\".\n",
    "\n",
    "**3. Conversão de tipo:**\n",
    "\n",
    "* Converte a coluna \"Temperature\" para o tipo numérico `float` para possibilitar cálculos.\n",
    "\n",
    "**4. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"City\".\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade utilizando funções Spark SQL.\n",
    "* Arredonda o valor médio da temperatura para duas casas decimais.\n",
    "\n",
    "**5. Ordenação:**\n",
    "\n",
    "* Ordena as estatísticas pela coluna \"City\".\n",
    "\n",
    "**6. Exibição de resultados:**\n",
    "\n",
    "* Imprime o DataFrame com as estatísticas na tela.\n",
    "\n",
    "**7. Encerramento da sessão Spark:**\n",
    "\n",
    "* Libera recursos utilizados pelo Spark.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento distribuído:** PySpark distribui o processamento por múltiplos nós em um cluster, permitindo lidar com conjuntos de dados massivos de maneira eficiente.\n",
    "* **Otimização para grandes dados:** PySpark oferece otimizações específicas para manipular grandes volumes de dados.\n",
    "* **Linguagem SQL familiar:** Usa Spark SQL para realizar consultas e transformações nos dados, facilitando a utilização para quem conhece SQL.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter o PySpark instalado e configurado corretamente.\n",
    "* O arquivo `data/measurements.txt` deve existir no caminho especificado.\n",
    "* A função `timer_to_csv` salva o tempo de execução em um arquivo CSV.\n",
    "\n",
    "**Documentação:** [pyspark](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd aula_05`\n",
    "2. **Execute o script:** `python src/pyspark_df.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44fe6fe-3fea-4d1a-b269-a1c4b91f8ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/29 02:50:19 WARN Utils: Your hostname, DESKTOP-AN64GAS resolves to a loopback address: 127.0.1.1; using 172.25.237.139 instead (on interface eth0)\n",
      "24/03/29 02:50:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/29 02:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+-----------+---------------+---------------+---------------+                   \n",
      "|       City|Max Temperature|Min Temperature|Avg Temperature|\n",
      "+-----------+---------------+---------------+---------------+\n",
      "|   A Coruña|           56.7|          -89.2|         -16.33|\n",
      "|   Aabenraa|           56.7|          -89.2|         -16.46|\n",
      "|      Aalen|           56.7|          -89.2|         -16.37|\n",
      "|   Aarschot|           56.7|          -89.2|         -16.03|\n",
      "| Aartselaar|           56.7|          -89.2|         -16.34|\n",
      "|    Aasiaat|           56.7|          -89.2|         -16.16|\n",
      "|     Abadan|           56.7|          -89.2|         -16.25|\n",
      "|     Abadou|           56.7|          -89.2|         -16.21|\n",
      "|     Abaeté|           56.7|          -89.2|          -16.2|\n",
      "|    Abarkūh|           56.7|          -89.2|         -16.12|\n",
      "|    Abasolo|           56.7|          -89.2|         -16.36|\n",
      "|  Abejorral|           56.7|          -89.2|         -16.14|\n",
      "|  Abensberg|           56.7|          -89.2|         -16.11|\n",
      "|   Abeokuta|           56.7|          -89.2|         -16.39|\n",
      "| Abhayāpuri|           56.7|          -89.2|         -16.43|\n",
      "|      Abhia|           56.7|          -89.2|         -16.24|\n",
      "|      Abiko|           56.7|          -89.2|         -16.26|\n",
      "|       Abim|           56.7|          -89.2|         -16.39|\n",
      "|   Abingdon|           56.7|          -89.2|         -16.29|\n",
      "|Abong Mbang|           56.7|          -89.2|         -16.27|\n",
      "+-----------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tempo total de execução: 3 minutos 42 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/pyspark_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57d311-2eca-4461-ad43-a5c43b81a97d",
   "metadata": {},
   "source": [
    "### Script pyspark_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3950ab52-82b6-4f9a-8a6b-d9afefc2aa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pyspark_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pyspark_df.py\n",
    "# Pyspark => script para caluclar min, max e mean em um bilhão de linhas.\n",
    "from utils.decorators import timer_to_csv\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.config import PATH\n",
    "from pyspark.sql.functions import col, min as spark_min, max as spark_max, avg as spark_avg, round as spark_round\n",
    "\n",
    "@timer_to_csv\n",
    "def pyspark_df(filename):     \n",
    "    # Inicializar uma sessão Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Temperature Analysis\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Ler o arquivo CSV diretamente em um DataFrame Spark\n",
    "    df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \";\").csv(filename) \\\n",
    "        .toDF(\"City\", \"Temperature\")\n",
    "    \n",
    "    # Converter a coluna 'Temperature' para tipo numérico\n",
    "    df = df.withColumn(\"Temperature\", col(\"Temperature\").cast(\"float\"))\n",
    "    \n",
    "    # Calcular estatísticas usando Spark SQL\n",
    "    statistics = df.groupBy(\"City\") \\\n",
    "        .agg(spark_max(\"Temperature\").alias(\"Max Temperature\"),\n",
    "             spark_min(\"Temperature\").alias(\"Min Temperature\"),             \n",
    "             spark_round(spark_avg(\"Temperature\"),2).alias(\"Avg Temperature\"))\n",
    "    \n",
    "    # Ordenar as estatísticas pela cidade\n",
    "    statistics_sorted = statistics.orderBy(\"City\")\n",
    "    \n",
    "    # Mostrar as estatísticas\n",
    "    return statistics_sorted.show()\n",
    "    \n",
    "    # Encerrar a sessão Spark\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = PATH\n",
    "    pyspark_df(filename)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fea63-3b69-4db9-9c95-0a2d833ae962",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2b54f-3cd0-4b33-9781-e1b2a8a2c66d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora08\"></a>\n",
    "## Vaex - min, max e mean em 1 bilhão de linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b2e662f-d655-4479-85d8-595a8d1455df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add vaex -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6590b-9d9e-40c3-822d-9f34d2cdf25a",
   "metadata": {},
   "source": [
    "### Documentação do Script Vaex:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef27e66-4075-4479-894c-12f67c9c3c26",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um grande conjunto de dados, possivelmente contendo um bilhão de linhas, utilizando a biblioteca Vaex. O Vaex é uma biblioteca Python especializada no processamento eficiente de grandes tabelas de dados.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` utilizando a função `vaex.from_csv`.\n",
    "* Define os nomes das colunas como \"city\" e \"temp\" e o separador como ponto-e-vírgula (;).\n",
    "\n",
    "**2. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" usando `df.groupby(df['city'])`.\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade utilizando a função `agg` com uma lista de funções de agregação.\n",
    "* Arredonda o valor médio da temperatura para duas casas decimais usando o método `round(2)`.\n",
    "\n",
    "**3. Exibição de resultados:**\n",
    "\n",
    "* A função `print` exibe o DataFrame final contendo as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento eficiente de grandes dados:** O Vaex é otimizado para lidar com grandes volumes de dados na memória, permitindo cálculos rápidos em tabelas com bilhões de linhas.\n",
    "* **Operações em memória:** Realiza a maioria das operações na memória principal, evitando acesso frequente ao disco e melhorando a performance.\n",
    "* **Uso intuitivo:** Oferece uma interface similar ao Pandas, facilitando a migração de scripts existentes.\n",
    "\n",
    "* **Documentação:** [vaex](https://vaex.io/docs/index.html)\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python vaex_df.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdd7daa-9809-4139-8c6c-774a09eda3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo decorrido: 7 minutos 38 segundos\r"
     ]
    }
   ],
   "source": [
    "!python src/vaex_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090a456-00d8-45aa-92c2-d9a348b8f384",
   "metadata": {},
   "source": [
    "### Script vaex_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc87d1a2-852f-4c65-bd6e-215faf967407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/vaex_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/vaex_df.py\n",
    "# Vaex => script para caluclar min, max e mean.\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "import vaex\n",
    "@timer_to_csv\n",
    "def vaex_df(filename):\n",
    "    # Leitura do arquivo CSV utilizando Vaex\n",
    "    df = vaex.from_csv(filename, sep=';', header=None, names=['city', 'temp'])\n",
    "\n",
    "    # Cálculo das estatísticas\n",
    "    combined_results = df.groupby(df['city']).agg({'temp': ['max', 'min', 'mean']})\n",
    "\n",
    "    # Ordenar por 'city'\n",
    "    combined_results = combined_results.sort(by='city')\n",
    "    \n",
    "    # Exibição dos resultados\n",
    "    return print(combined_results)\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    filename = PATH\n",
    "    vaex_df(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd23169-51ee-422f-b3ab-3012277fb768",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e549a-a6eb-47fb-af6c-273174f288c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora09\"></a>\n",
    "## Cudf com pandas via GPU - min, max e mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c13e7-9825-4fa1-8f83-f38d469bfa91",
   "metadata": {},
   "source": [
    "### Para utilizar precisa ter uma GPU compatível e o DriveToolKit instalado conforme o link da [Nvidia](https://developer.nvidia.com/cuda-gpus#compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc3bc5-e731-4f50-b003-294fc982ecef",
   "metadata": {},
   "source": [
    "* **Confira a versão cuda instalado com o comando abaixo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee832a0-7f24-4968-b5d2-7a3122d6ea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 29 03:02:33 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:03:00.0  On |                  N/A |\n",
      "|  0%   50C    P8             15W /  200W |     635MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        36      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c319fba-02a7-44be-99df-a52833f8fdda",
   "metadata": {},
   "source": [
    "* **No meu caso estou com a versão 12+ com essa informação acesse o site para escolher a versão correta no site:**\n",
    "[RAPIDS-Nvidi](https://docs.rapids.ai/install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab3c819-0818-4ee5-b9a0-4ab180eb3384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "#verificando a versão do python para esse estudo.\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478424c-7e70-4ff2-be3e-13b107d83c98",
   "metadata": {},
   "source": [
    "* **Verificando se o kernel está setado no ambiente virtual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8e6ce99-02f6-470c-99fb-40a2bf5f481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from site import getsitepackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1181f64-c461-4fa5-b99c-176c27edf4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d74a151a-18d2-4e45-aa01-6908082eca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "#Necessário fazer upgrade do pip\n",
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40db29da-70d6-40bb-a794-462547390d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-expr 1.0.5 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalação somente pelo pip. não funciona com poetry add.\n",
    "!pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com cudf-cu12 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee476b7-5b26-496a-b13d-af82c876c767",
   "metadata": {},
   "source": [
    "#### Após a instalação é necessário reiniciar o kernel do jupyter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944e509b-adab-469c-9d48-41314e54020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reinicia o kernel jupyter\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c817ba4-9ac3-46d4-b8e9-df44b484972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Conferir e selecionar a pasta raiz do projeto.\n",
    "os.getcwd()\n",
    "os.chdir('/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4958c6-2438-4799-8da9-35c76244cbd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                city   max   min       mean\n",
      "0           A Coruña  56.7 -89.2 -15.912123\n",
      "1           Aabenraa  56.7 -89.1 -16.448598\n",
      "2              Aalen  56.7 -89.2 -16.864149\n",
      "3           Aarschot  56.7 -89.2 -15.868667\n",
      "4         Aartselaar  56.7 -89.2 -16.101908\n",
      "...              ...   ...   ...        ...\n",
      "89025         ‘Aqrah  56.7 -89.2 -15.888105\n",
      "89026  ‘Ayn al ‘Arab  56.7 -89.2 -15.478279\n",
      "89027          ‘Ibrī  56.7 -89.2 -16.310798\n",
      "89028     ’Aïn Arnat  56.7 -89.2 -16.459072\n",
      "89029      ’Aïn Roua  56.7 -89.2 -15.758870\n",
      "\n",
      "[89030 rows x 4 columns]\n",
      "Tempo total de execução: 11 minutos 43 segundos\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/cudf_pandas_df.py\n",
    "# CuDF_Pandas=> script para caluclar min, max e mean em um bilhão de linhas na GPU rtx 3060ti.\n",
    "# Necessário possuir uma GPU compativel e instalar o pacote RAPIDS da Nvidia.\n",
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "from src.utils.decorators import timer_to_csv\n",
    "@timer_to_csv\n",
    "def cudf_pandas_df(file_path, chunk_size):              \n",
    "    # Inicialize um DataFrame vazio para armazenar os resultados\n",
    "    results_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop através do arquivo em chunks e calcule as estatísticas\n",
    "    for chunk in pd.read_csv(file_path,sep=';', header=None, names=['city', 'temp'], chunksize=chunk_size):\n",
    "        # Calcular as estatísticas por grupo\n",
    "        grouped_stats = chunk.groupby('city').agg({'temp': ['max','min','mean']}).reset_index()\n",
    "        # Concatenar os resultados ao DataFrame principal\n",
    "        results_df = pd.concat([results_df, grouped_stats], ignore_index=True)\n",
    "    # Removendo o level city    \n",
    "    results_df = results_df.droplevel(0,axis=1)\n",
    "    # Renomeando o level 0 para city\n",
    "    results_df.rename(columns={'':'city'}, inplace=True)\n",
    "    # Fazendo o groupby geral.\n",
    "    results_df.groupby('city').agg({ 'max':'max', 'min':'min', 'mean':'mean'}).round(2).sort_values('city')    \n",
    "    # Resultados finais\n",
    "    return print(results_df)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    # Defina o caminho para o seu arquivo CSV\n",
    "    file_path = 'data/measurements.txt'\n",
    "    # Tamanho do chunk (você pode ajustar conforme necessário)\n",
    "    chunk_size = 100_000_000  # por exemplo, 1 milhão de linhas por chunk\n",
    "    cudf_pandas_df(file_path,chunk_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ccf0a-673e-41db-a54f-973aacb0678d",
   "metadata": {},
   "source": [
    "### Documentação do Script cudf_pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e02e54-5975-4862-b7fe-275f37311550",
   "metadata": {},
   "source": [
    "* **Para a leitura para 10 milhões de linha foi extremamente rápido, porém para 1 bilhão a memória não suportou.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c2c6b-e636-4db9-8a26-e648edd1f6b8",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados utilizando a biblioteca CuDF. O CuDF é uma biblioteca similar ao Pandas, porém otimizada para processamento em GPUs, possibilitando um desempenho significativamente superior.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "Este script **não é recomendado** para processar um bilhão de linhas, pois a memória da GPU pode ser insuficiente. Para conjuntos de dados massivos, utilize a versão do script CuDF otimizada para Dask.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura de dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` usando `cudf.read_csv`.\n",
    "* Define `header=None` pois o arquivo não possui cabeçalho e `sep=;` como separador.\n",
    "* Cria as colunas \"city\" e \"temp\".\n",
    "\n",
    "**2. Cálculo de estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" com `df.groupby('city')`.\n",
    "* Calcula o mínimo, máximo e média da temperatura para cada cidade usando `agg` com uma lista de funções de agregação.\n",
    "\n",
    "**3. Ordenação:**\n",
    "\n",
    "* Ordena o DataFrame pelo índice (cidade) usando `sort_index()`.\n",
    "\n",
    "**4. Exibição de resultados:**\n",
    "\n",
    "* Imprime o DataFrame final com as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento em GPU:** O CuDF utiliza a GPU para realizar cálculos, proporcionando um aumento significativo de performance em comparação ao Pandas em CPUs.\n",
    "* **Interface familiar:** A API do CuDF é similar ao Pandas, facilitando a migração de scripts existentes.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter o CuDF instalado e drivers Nvidia atualizados.\n",
    "* O arquivo `data/measurements_pandas.txt` deve existir no caminho especificado.\n",
    "* A memória da GPU pode ser insuficiente para processar grandes conjuntos de dados.\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python cudf_pandas_df.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728394c5-bccc-4f4d-8587-9499701918dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  temp                 \n",
      "                   min   max       mean\n",
      "city                                   \n",
      "A Coruña         -87.5  56.7 -17.676995\n",
      "A Yun Pa         -88.8  54.7 -18.562727\n",
      "Aabenraa         -89.1  56.4 -15.115238\n",
      "Aachen           -88.1  56.6 -15.190476\n",
      "Aadorf           -87.9  56.1 -18.672059\n",
      "...                ...   ...        ...\n",
      "’Tlat Bni Oukil  -89.2  56.6 -17.527602\n",
      "’s-Gravendeel    -88.5  56.5 -13.676190\n",
      "’s-Gravenzande   -88.9  56.7 -13.000493\n",
      "’s-Heerenberg    -87.8  56.2 -18.259174\n",
      "’s-Hertogenbosch -86.6  55.4 -12.958454\n",
      "\n",
      "[41343 rows x 3 columns]\n",
      "Tempo total de execução: 0.820 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/cudf_pandas_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c806c-f11f-49c2-bafe-1a1321c70081",
   "metadata": {},
   "source": [
    "### Script cudf_pandas_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9a4784-5d7a-4e9f-abb4-b2e69fee5fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cudf_pandas_df.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cudf_pandas_df.py\n",
    "# Script usando cudf == pandas mas rápido pq usa a GPU, porém a mem não suport procesar 1bilhão.\n",
    "import cudf\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "@timer_to_csv\n",
    "def cudf_pandas(filename):\n",
    "    \n",
    "    # Carregar o arquivo CSV e criar os cabeçalhos 'city' e 'temp'\n",
    "    df = cudf.read_csv(filename, header=None, sep=';', names=['city', 'temp'])\n",
    "    \n",
    "    # Agrupar por 'city' e calcular o 'min', 'max' e 'mean' da coluna 'temp'\n",
    "    grouped_df = df.groupby('city').agg({'temp': ['min', 'max', 'mean']})\n",
    "    \n",
    "    # Ordenar o DataFrame pelo índice (city)\n",
    "    result = grouped_df.sort_index()     \n",
    "    \n",
    "    # retorna o resultado\n",
    "    return print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = PATH\n",
    "    cudf_pandas(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca0932-bde2-4bde-9b69-9827f3f167e1",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c59e4-a3d7-4caa-a681-0e05cae2b033",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora10\"></a>\n",
    "## Dask_cudf com GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718461e-b026-4ea5-be22-3836c6f8a000",
   "metadata": {},
   "source": [
    "#### Para utilização do dasck_cudf será necessária a instalação via pip conforme abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6391c04c-ecc0-49f5-adbc-5be0e6977e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-expr 1.0.5 requires dask==2024.3.1, but you have dask 2024.1.1 which is incompatible.\n",
      "dask-expr 1.0.5 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalação da lib via pip, via poetry apresentou erros.\n",
    "!pip install --no-cache-dir --extra-index-url=https://pypi.nvidia.com dask-cudf-cu12 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2cca96e-88cd-4d2a-8767-402059ce0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da lib dask_cuda\n",
    "!pip install dask-cuda -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "685f397b-fcd1-4b90-a80b-3d3f981eb23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reinicia o kernel jupyter\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd753286-6f3e-4b4b-af80-b9563ed671c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Conferir e selecionar a pasta raiz do projeto.\n",
    "os.getcwd()\n",
    "os.chdir('/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/aula_05')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cd094-5ec9-4d16-b0cf-54dfb4f8ba66",
   "metadata": {},
   "source": [
    "### Documentação do Script Dask-CuDF com Blocksize 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048e863-feec-42e8-b894-679a90bd7ea0",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script demonstra como calcular a temperatura mínima, máxima e média para cada cidade em um conjunto de dados de um bilhão de linhas utilizando Dask-CuDF, combinando o poder do processamento paralelo do Dask com a aceleração por GPU do CuDF.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Configuração do Cluster GPU:**\n",
    "\n",
    "* Cria um cluster local com um único nó utilizando a GPU disponível (`LocalCUDACluster`).\n",
    "* Inicia um cliente Dask conectado ao cluster para distribuir o processamento.\n",
    "\n",
    "**2. Leitura dos Dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/measurements_pandas.txt` usando `dc.read_csv`.\n",
    "* Define as seguintes opções:\n",
    "    - `sep=';'` para indicar o separador de colunas.\n",
    "    - `header=None` pois o arquivo não possui cabeçalho.\n",
    "    - `names=['city', 'temp']` para nomear as colunas.\n",
    "    - `dtype={'city': 'str', 'temp': 'float32'}` para especificar os tipos de dados.\n",
    "    - `blocksize='1024 MiB'` para dividir o arquivo em partições de 1024 MiB para processamento paralelo.\n",
    "\n",
    "**3. Cálculo de Estatísticas:**\n",
    "\n",
    "* Agrupa o DataFrame por \"city\" usando `df.groupby('city')`.\n",
    "* Calcula o máximo, mínimo e média da temperatura para cada cidade usando `agg` com uma lista de funções de agregação.\n",
    "* Computa os resultados finais com `compute()`.\n",
    "* Ordena o DataFrame pelo índice (cidade) usando `sort_index()`.\n",
    "\n",
    "**4. Exibição de Resultados:**\n",
    "\n",
    "* Imprime o DataFrame final contendo as estatísticas na tela.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "* **Processamento em GPU Distribuído:** Combina Dask para paralelismo com CuDF para aceleração por GPU.\n",
    "* **Blocksize para Particionamento:** Divide o arquivo em partições para melhor processamento paralelo.\n",
    "* **Interface Familiar:** API similar ao Pandas, facilitando a adaptação.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* Certifique-se de ter Dask, Dask-CUDA e CuDF instalados.\n",
    "* A GPU deve ter memória suficiente para processar as partições de 1024 MiB.\n",
    "* O arquivo `data/measurements_pandas.txt` deve existir no caminho especificado.\n",
    "\n",
    "**Execução:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python dask_cudf_1024.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17202a20-0707-4b56-9533-50020e4b171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n",
      "                    temp                      \n",
      "                     max        min       mean\n",
      "city                                          \n",
      "A Coruña       56.700001 -89.199997 -16.325014\n",
      "Aabenraa       56.700001 -89.199997 -16.461449\n",
      "Aalen          56.700001 -89.199997 -16.367747\n",
      "Aarschot       56.700001 -89.199997 -16.028289\n",
      "Aartselaar     56.700001 -89.199997 -16.336841\n",
      "...                  ...        ...        ...\n",
      "‘Aqrah         56.700001 -89.199997 -16.316323\n",
      "‘Ayn al ‘Arab  56.700001 -89.199997 -16.092673\n",
      "‘Ibrī          56.700001 -89.199997 -16.423083\n",
      "’Aïn Arnat     56.700001 -89.199997 -16.309802\n",
      "’Aïn Roua      56.700001 -89.199997 -16.148490\n",
      "\n",
      "[8903 rows x 3 columns]\n",
      "Tempo total de execução: 1 minutos 41 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_cudf_1024.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37452434-65a6-43ac-810f-17489ac9e589",
   "metadata": {},
   "source": [
    "### Script Dasck_cudf_1024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df064c24-11ad-4298-9d30-69ea19d15698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/dask_cudf_1024.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_cudf_1024.py\n",
    "# dask_cudf com blocksize 1024 => script para calcular min, max e mean em um bilhão de linhas usando a gpu rtx 3060ti.\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "import dask_cudf as dc\n",
    "\n",
    "@timer_to_csv\n",
    "def dask_cudf_1024(filename):\n",
    "    # Definindo o cluster para usar gpu\n",
    "    with LocalCUDACluster() as cluster, Client(cluster) as client:\n",
    "        # Carregar o arquivo csv em um dataframe dask_cudf\n",
    "        df = dc.read_csv(filename,\n",
    "                         sep=';', header=None,\n",
    "                         names=['city', 'temp'], \n",
    "                         dtype={'city': 'str', 'temp': 'float32'},\n",
    "                         blocksize='1024 MiB')\n",
    "        \n",
    "        # Agrupar pela coluna 'city' e calcular min, max e mean da coluna 'temp'\n",
    "        result = df.groupby('city').agg({'temp': ['max','min','mean']}).compute().sort_index()\n",
    "        print(result)\n",
    "if __name__ == \"__main__\":\n",
    "    filename= PATH\n",
    "    dask_cudf_1024(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca0eee-5817-4d62-8913-c5cfec1500b4",
   "metadata": {},
   "source": [
    "### Script Dask_cudf_256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e030d72-2b7f-4edf-a3d6-475a70e92766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/dask_cudf_256.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/dask_cudf_256.py\n",
    "# dask_cudf com blocksize 256mb => script para caluclar min, max e mean em um bilhão de linhas usando a gpu rtx 3060ti.\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from utils.decorators import timer_to_csv\n",
    "from utils.config import PATH\n",
    "import dask_cudf as dc\n",
    "\n",
    "@timer_to_csv\n",
    "def dask_cudf_256(filename):\n",
    "    # Definindo o cluster para usar gpu\n",
    "    with LocalCUDACluster() as cluster, Client(cluster) as client:\n",
    "        # Carregar o arquivo csv em um dataframe dask_cudf\n",
    "        df = dc.read_csv(filename,\n",
    "                         sep=';', header=None,\n",
    "                         names=['city', 'temp'], \n",
    "                         dtype={'city': 'str', 'temp': 'float32'},\n",
    "                         blocksize='256 MiB')\n",
    "        \n",
    "        # Agrupar pela coluna 'city' e calcular min, max e mean da coluna 'temp'\n",
    "        result = df.groupby('city').agg({'temp': ['max','min','mean']}).compute().sort_index()\n",
    "        print(result)\n",
    "if __name__ == \"__main__\":\n",
    "    filename= PATH\n",
    "    dask_cudf_256(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb4cdda-35e1-402e-b29e-4e6ab2455b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcnok/bootcamps/mytests/meus_projetos/bootcamp-jornada-de-dados_2024/.venv/lib/python3.10/site-packages/dask_cuda/utils.py:170: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  warnings.warn(\n",
      "                    temp                      \n",
      "                     max        min       mean\n",
      "city                                          \n",
      "A Coruña       56.700001 -89.199997 -16.325012\n",
      "Aabenraa       56.700001 -89.199997 -16.461450\n",
      "Aalen          56.700001 -89.199997 -16.367746\n",
      "Aarschot       56.700001 -89.199997 -16.028291\n",
      "Aartselaar     56.700001 -89.199997 -16.336843\n",
      "...                  ...        ...        ...\n",
      "‘Aqrah         56.700001 -89.199997 -16.316331\n",
      "‘Ayn al ‘Arab  56.700001 -89.199997 -16.092675\n",
      "‘Ibrī          56.700001 -89.199997 -16.423089\n",
      "’Aïn Arnat     56.700001 -89.199997 -16.309804\n",
      "’Aïn Roua      56.700001 -89.199997 -16.148494\n",
      "\n",
      "[8903 rows x 3 columns]\n",
      "Tempo total de execução: 53.465 segundos\n"
     ]
    }
   ],
   "source": [
    "!python src/dask_cudf_256.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd4657-f16a-4687-8bd7-7dc1e65a2c35",
   "metadata": {},
   "source": [
    "* **Alterando para 256 MiB a performance melhorou, mas ainda assim bem longe de superar o DuckDB.**\n",
    "* **Conforme o benchmarch da documentação ele tem capacidade de superar o sgdb. Talvez alguma configuração pois ainda não li toda a documentação.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a90f3-1d85-4fb0-bad7-3ce795a1beab",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac218d-60f5-4f0c-bb05-886f74fef005",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora11\"></a>\n",
    "## Gerando um Gráfico dos Resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67997a8d-42a5-40bd-928a-02614905e4fd",
   "metadata": {},
   "source": [
    "#### Segue abaixo o resultado do ranking geral de todos os testes realizados para calcular o min, max e média ordenado pela cidade em um bilhão de linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5469e-a8c2-4139-83ca-5a6b57519cd1",
   "metadata": {},
   "source": [
    "### Resultado realizado para calcular o min, max e média ordenado pela cidade em 10 Milhões de linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a578ea-1b3d-428a-a23c-abedc2d7108c",
   "metadata": {},
   "source": [
    "![png](./img/ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22e866-e7f7-49b3-930b-408535bcdcc7",
   "metadata": {},
   "source": [
    "* **obs:** Vale ressaltar que esse resultado só serve de parâmetro para minha máquina, com base nos scripts gerados aqui, certamente cabem espaços para otimização e em outros cenários os resultados podem ser bem divergentes.\n",
    "* segue as configurações da minha máquina abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c7c2e-4c8e-41ee-bcc1-c0834c01cbad",
   "metadata": {},
   "source": [
    "#### Memória:\n",
    "Memory block size:       128M\n",
    "Total online memory:      12G\n",
    "Total offline memory:      0B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d37301-3900-45b5-b5a0-09c6b6f52210",
   "metadata": {},
   "source": [
    "#### CPU:\n",
    "Architecture:            x86_64\n",
    "  CPU op-mode(s):        32-bit, 64-bit\n",
    "  Address sizes:         46 bits physical, 48 bits virtual\n",
    "  Byte Order:            Little Endian\n",
    "CPU(s):                  24\n",
    "  On-line CPU(s) list:   0-23\n",
    "Vendor ID:               GenuineIntel\n",
    "  Model name:            Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\n",
    "    CPU family:          6\n",
    "    Model:               63\n",
    "    Thread(s) per core:  2\n",
    "    Core(s) per socket:  12\n",
    "    Socket(s):           1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03902619-8afb-4d6d-bd2a-18f084a93113",
   "metadata": {},
   "source": [
    "#### GPU:\n",
    "RTX 3060ti 8Gbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6a1b93-b6af-49f4-b4bb-a059547f723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001b[36mmatplotlib\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "# Instalação da lib para plotar o grafico.\n",
    "!poetry add matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaf088-18a8-478d-98c5-498f65a52d55",
   "metadata": {},
   "source": [
    "### Documentação do Script: Plotando Ranking de Tempo de Execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde5646-0f00-4937-9142-4a6a3a3071ed",
   "metadata": {},
   "source": [
    "**Descrição:**\n",
    "\n",
    "Este script Python gera um gráfico de barras que mostra o ranking das funções em relação ao tempo de execução, ordenando-as do menor para o maior tempo.\n",
    "\n",
    "**Funcionalidade:**\n",
    "\n",
    "**1. Leitura dos Dados:**\n",
    "\n",
    "* Lê o arquivo CSV `data/tempos_execucao.csv` que contém o nome da função e o tempo de execução (em segundos).\n",
    "* Cada linha do arquivo CSV deve conter:\n",
    "    - Nome da função (string).\n",
    "    - Tempo de execução no formato `X segundos` ou `Y minutos e Z segundos`.\n",
    "\n",
    "**2. Extração e Conversão:**\n",
    "\n",
    "* Extrai o nome da função e o tempo de execução de cada linha.\n",
    "* Converte o tempo de execução para segundos, considerando minutos e segundos.\n",
    "* Armazena os dados em uma lista de tuplas (`(nome_funcao, tempo_total)`).\n",
    "\n",
    "**3. Ordenação:**\n",
    "\n",
    "* Ordena a lista de tuplas pelo tempo de execução (tempo_total) em ordem crescente.\n",
    "\n",
    "**4. Plotagem do Gráfico:**\n",
    "\n",
    "* Cria um gráfico de barras com a biblioteca `matplotlib`.\n",
    "* As funções (ordenadas) são dispostas no eixo Y e o tempo de execução (em segundos) no eixo X.\n",
    "* As barras são coloridas em azul celeste.\n",
    "* O título do gráfico indica \"Tempo de Execução das Funções (Ordenado)\".\n",
    "* O eixo X é rotulado como \"Tempo de Execução (segundos)\" e o eixo Y como \"Função\".\n",
    "* Uma grade é adicionada ao eixo X para facilitar a leitura.\n",
    "* O tempo de execução de cada função é exibido acima da barra respectiva.\n",
    "\n",
    "**5. Salva o Gráfico:**\n",
    "\n",
    "* Salva o gráfico para visualização dos resultados na pasta img/ranking.png.\n",
    "\n",
    "**Observações:**\n",
    "\n",
    "* O arquivo `data/tempos_execucao.csv` deve existir no caminho especificado.\n",
    " \n",
    "**Exemplo de Uso:**\n",
    "\n",
    "1. **Navegue até o diretório do script:** `cd src`\n",
    "2. **Execute o script:** `python plot_ranking.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fea12fb-0f2e-4e17-a6b2-f9be973fa004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráfico salvo com sucesso na pasta:img/ranking.png\n"
     ]
    }
   ],
   "source": [
    "!python src/plot_ranking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43440cbb-fb75-4aed-ac86-07ba8a6fe5ce",
   "metadata": {},
   "source": [
    "### Script para plotar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8841ca81-aeae-4ab6-866f-0651b6526b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/plot_ranking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/plot_ranking.py\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ranking_tempo_execucao(arquivo_csv):\n",
    "    # Leitura dos dados do arquivo CSV\n",
    "    dados = []\n",
    "\n",
    "    with open(arquivo_csv, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            nome_funcao = row[0]\n",
    "            tempo = row[1].split()  # Separa minutos e segundos\n",
    "            if len(tempo) == 2 and tempo[1] == 'segundos':  # Se houver apenas segundos\n",
    "                tempo_total = float(tempo[0])\n",
    "            elif len(tempo) == 4:  # Se houver minutos e segundos\n",
    "                minutos = float(tempo[0])\n",
    "                segundos = float(tempo[2])\n",
    "                tempo_total = minutos * 60 + segundos  # Converte minutos para segundos\n",
    "            else:\n",
    "                raise ValueError(f'Formato de tempo inválido: {row[1]}')\n",
    "            dados.append((nome_funcao, tempo_total))\n",
    "\n",
    "    # Ordena os dados pelo tempo de execução\n",
    "    dados_ordenados = sorted(dados, key=lambda x: x[1])\n",
    "\n",
    "    # Extrai os nomes das funções e os tempos de execução ordenados\n",
    "    nomes_funcoes_ordenados = [item[0] for item in dados_ordenados]\n",
    "    tempos_execucao_ordenados = [item[1] for item in dados_ordenados]\n",
    "\n",
    "    # Plotagem do gráfico\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nomes_funcoes_ordenados, tempos_execucao_ordenados, color='skyblue')\n",
    "    plt.xlabel('Tempo de Execução (segundos)')\n",
    "    plt.ylabel('Função')\n",
    "    plt.title('Tempo de Execução das Funções (Ordenado)')\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adicionando os valores nas barras\n",
    "    for i, valor in enumerate(tempos_execucao_ordenados):\n",
    "        plt.text(valor, i, f'{valor:.2f} s', va='center')\n",
    "\n",
    "    # Exibindo o gráfico\n",
    "    # plt.show()\n",
    "    plt.savefig('./img/ranking.png')\n",
    "    print('Gráfico salvo com sucesso na pasta:img/ranking.png')\n",
    "\n",
    "# Exemplo de uso da função\n",
    "if __name__ == \"__main__\":\n",
    "    arquivo_csv = './data/tempos_execucao.csv'\n",
    "    plot_ranking_tempo_execucao(arquivo_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb930662-c816-423c-a603-1a53c16c87be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráfico salvo com sucesso na pasta:img/ranking.png\n"
     ]
    }
   ],
   "source": [
    "!python src/utils/plot_ranking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f99a3-28c2-4343-bc35-c1c62a497d8a",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195bb88-2ff7-4014-8b93-826d513042e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"ancora12\"></a>\n",
    "## Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8f955-a049-4d86-b025-13ebe144a009",
   "metadata": {},
   "source": [
    "![png](./img/ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556f126-9271-41c5-889d-d82117828ae1",
   "metadata": {},
   "source": [
    "#### O projeto em questão consistiu em um estudo comparativo de desempenho entre diferentes bibliotecas de processamento de dados em Python, utilizando uma máquina com configurações específicas. As bibliotecas testadas foram o pandas, dask, pyspark, polars, duckdb, cudf (cuDF), dask_cudf (Dask cuDF), e vaex. \n",
    "#### Este estudo teve como objetivo avaliar a eficiência de cada biblioteca em um cenário de processamento de dados, considerando a utilização de uma única CPU Intel E5-2670 v3 com 12Gb de memória e uma GPU NVIDIA RTX 3060 Ti com 8Gb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d08ab-59d6-43e0-ae4d-e28d25781f9a",
   "metadata": {},
   "source": [
    "#### Os resultados obtidos foram variados, com o duckdb se destacando com o melhor desempenho, completando o script em aproximadamente 23,52 segundos. \n",
    "#### O polars, outra biblioteca de processamento de dados, executou o script em 44,29 segundos, seguido pelo dask_cudf, que, apesar de exigir pelo menos uma GPU compatível, demonstrou potencial para processamento em paralelo e distribuído, terminando o script em 53,47 segundos com partições de 256MB. \n",
    "#### O pyspark, que também se beneficia do processamento distribuído, ficou em quarto lugar, e o dask, que é uma biblioteca de paralelismo e distribuição genérica para Python, ficou em quinto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454811a7-493f-4274-a028-13a1ba07d059",
   "metadata": {},
   "source": [
    "#### O pandas e o cuDF, versões do pandas otimizadas para GPUs, tiveram um desempenho que variou de acordo com a quantidade de dados, não conseguindo processar todos os dados na memória e, por isso, foram divididos em partes (chunks). O vaex, outra biblioteca para trabalhar com grandes volumes de dados não suportou processar a quantidade de dados, certamente por eu não ter me aprofundado mais em sua documentação, porém mostraram-se muito eficientes com conjuntos de dados menores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752d0a5-8238-468e-938d-7322db123235",
   "metadata": {},
   "source": [
    "#### Com base nesses resultados, é possível concluir que a escolha da biblioteca de processamento de dados ideal depende muito do caso específico e das condições de hardware disponíveis. Por exemplo, o duckdb pode ser a melhor opção para processamento em uma única máquina, enquanto o pyspark pode ser mais adequado para cenários de processamento distribuído em várias máquinas. O dask_cudf, por sua vez, pode ser uma escolha poderosa se houver mais de uma GPU disponível.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16ede1-7ebd-4fcc-a846-04b3e6ec50d7",
   "metadata": {},
   "source": [
    "#### Portanto, é crucial estar atento às novas ferramentas e tecnologias, já que o desempenho e a adequação de cada biblioteca podem variar significativamente de acordo com as necessidades do projeto e os recursos computacionais disponíveis. Este estudo destaca a importância de realizar testes de desempenho com as bibliotecas específicas para o caso em questão, a fim de tomar a melhor decisão para o projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1775b-0922-4a44-9090-5392326e264f",
   "metadata": {},
   "source": [
    "[voltar](#voltar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a75205-7d6f-469d-af91-9bfea70280ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
